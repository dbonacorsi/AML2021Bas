{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"AML_7_Modelling_AlgoPerfMetrics.ipynb","provenance":[],"collapsed_sections":["MrMybQseH_hO","BKNM45TQH_hX","dXrSnEVEH_hb"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"G-txJJP7H_hJ"},"source":["# ML Algorithm Performance Metrics"]},{"cell_type":"markdown","metadata":{"id":"oarLVuEcH_hL"},"source":["## Algorithm Evaluation Metrics"]},{"cell_type":"markdown","metadata":{"id":"2M18Wz-HIk5G"},"source":["We need more input datasets:\n","* For CLASSIFICATION metrics: we use the Pima Indians onset of diabetes dataset\n","* For REGRESSION metrics: we use the Boston House Price dataset \n"]},{"cell_type":"markdown","metadata":{"id":"we0oQqeDJSSZ"},"source":["We do not focus on modelling utself, in this notebook, so we use ***Logistic Regression*** for the classification problem and ***Linear Regression*** for the regression problems. \n","\n","A 10-fold CV test harness is used to demonstrate each metric (because this is a  likely scenario you will use when employing different algorithm evaluation metrics)\n","\n","More about ML algorithm performance metrics supported by scikit-learn can be found [here](http://scikit-learn.org/stable/modules/model_evaluation.html) on the page \"Model evaluation: quantifying the quality of predictions\". "]},{"cell_type":"markdown","metadata":{"id":"S6L3iCx6H_hM"},"source":["# CLASSIFICATION Metrics"]},{"cell_type":"markdown","metadata":{"id":"Fy2LelSIPnS1"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"D_0zvG3kPnJl","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1617856586445,"user_tz":-120,"elapsed":1005,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"b025f09b-fceb-4e90-e4d7-69c378cf91f4"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML2021Bas/main/datasets/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     preg  plas  pres  skin  test  mass   pedi  age  class\n","0       6   148    72    35     0  33.6  0.627   50      1\n","1       1    85    66    29     0  26.6  0.351   31      0\n","2       8   183    64     0     0  23.3  0.672   32      1\n","3       1    89    66    23    94  28.1  0.167   21      0\n","4       0   137    40    35   168  43.1  2.288   33      1\n","5       5   116    74     0     0  25.6  0.201   30      0\n","6       3    78    50    32    88  31.0  0.248   26      1\n","7      10   115     0     0     0  35.3  0.134   29      0\n","8       2   197    70    45   543  30.5  0.158   53      1\n","9       8   125    96     0     0   0.0  0.232   54      1\n","10      4   110    92     0     0  37.6  0.191   30      0\n","11     10   168    74     0     0  38.0  0.537   34      1\n","12     10   139    80     0     0  27.1  1.441   57      0\n","13      1   189    60    23   846  30.1  0.398   59      1\n","14      5   166    72    19   175  25.8  0.587   51      1\n","15      7   100     0     0     0  30.0  0.484   32      1\n","16      0   118    84    47   230  45.8  0.551   31      1\n","17      7   107    74     0     0  29.6  0.254   31      1\n","18      1   103    30    38    83  43.3  0.183   33      0\n","19      1   115    70    30    96  34.6  0.529   32      1\n","20      3   126    88    41   235  39.3  0.704   27      0\n","21      8    99    84     0     0  35.4  0.388   50      0\n","22      7   196    90     0     0  39.8  0.451   41      1\n","23      9   119    80    35     0  29.0  0.263   29      1\n","24     11   143    94    33   146  36.6  0.254   51      1\n","25     10   125    70    26   115  31.1  0.205   41      1\n","26      7   147    76     0     0  39.4  0.257   43      1\n","27      1    97    66    15   140  23.2  0.487   22      0\n","28     13   145    82    19   110  22.2  0.245   57      0\n","29      5   117    92     0     0  34.1  0.337   38      0\n","..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n","738     2    99    60    17   160  36.6  0.453   21      0\n","739     1   102    74     0     0  39.5  0.293   42      1\n","740    11   120    80    37   150  42.3  0.785   48      1\n","741     3   102    44    20    94  30.8  0.400   26      0\n","742     1   109    58    18   116  28.5  0.219   22      0\n","743     9   140    94     0     0  32.7  0.734   45      1\n","744    13   153    88    37   140  40.6  1.174   39      0\n","745    12   100    84    33   105  30.0  0.488   46      0\n","746     1   147    94    41     0  49.3  0.358   27      1\n","747     1    81    74    41    57  46.3  1.096   32      0\n","748     3   187    70    22   200  36.4  0.408   36      1\n","749     6   162    62     0     0  24.3  0.178   50      1\n","750     4   136    70     0     0  31.2  1.182   22      1\n","751     1   121    78    39    74  39.0  0.261   28      0\n","752     3   108    62    24     0  26.0  0.223   25      0\n","753     0   181    88    44   510  43.3  0.222   26      1\n","754     8   154    78    32     0  32.4  0.443   45      1\n","755     1   128    88    39   110  36.5  1.057   37      1\n","756     7   137    90    41     0  32.0  0.391   39      0\n","757     0   123    72     0     0  36.3  0.258   52      1\n","758     1   106    76     0     0  37.5  0.197   26      0\n","759     6   190    92     0     0  35.5  0.278   66      1\n","760     2    88    58    26    16  28.4  0.766   22      0\n","761     9   170    74    31     0  44.0  0.403   43      1\n","762     9    89    62     0     0  22.5  0.142   33      0\n","763    10   101    76    48   180  32.9  0.171   63      0\n","764     2   122    70    27     0  36.8  0.340   27      0\n","765     5   121    72    23   112  26.2  0.245   30      0\n","766     1   126    60     0     0  30.1  0.349   47      1\n","767     1    93    70    31     0  30.4  0.315   23      0\n","\n","[768 rows x 9 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>preg</th>\n","      <th>plas</th>\n","      <th>pres</th>\n","      <th>skin</th>\n","      <th>test</th>\n","      <th>mass</th>\n","      <th>pedi</th>\n","      <th>age</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>116</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>25.6</td>\n","      <td>0.201</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3</td>\n","      <td>78</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>88</td>\n","      <td>31.0</td>\n","      <td>0.248</td>\n","      <td>26</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>10</td>\n","      <td>115</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>35.3</td>\n","      <td>0.134</td>\n","      <td>29</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>197</td>\n","      <td>70</td>\n","      <td>45</td>\n","      <td>543</td>\n","      <td>30.5</td>\n","      <td>0.158</td>\n","      <td>53</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>8</td>\n","      <td>125</td>\n","      <td>96</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.232</td>\n","      <td>54</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4</td>\n","      <td>110</td>\n","      <td>92</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>37.6</td>\n","      <td>0.191</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>10</td>\n","      <td>168</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>38.0</td>\n","      <td>0.537</td>\n","      <td>34</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>10</td>\n","      <td>139</td>\n","      <td>80</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>27.1</td>\n","      <td>1.441</td>\n","      <td>57</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>189</td>\n","      <td>60</td>\n","      <td>23</td>\n","      <td>846</td>\n","      <td>30.1</td>\n","      <td>0.398</td>\n","      <td>59</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>5</td>\n","      <td>166</td>\n","      <td>72</td>\n","      <td>19</td>\n","      <td>175</td>\n","      <td>25.8</td>\n","      <td>0.587</td>\n","      <td>51</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>7</td>\n","      <td>100</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.0</td>\n","      <td>0.484</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0</td>\n","      <td>118</td>\n","      <td>84</td>\n","      <td>47</td>\n","      <td>230</td>\n","      <td>45.8</td>\n","      <td>0.551</td>\n","      <td>31</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7</td>\n","      <td>107</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29.6</td>\n","      <td>0.254</td>\n","      <td>31</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1</td>\n","      <td>103</td>\n","      <td>30</td>\n","      <td>38</td>\n","      <td>83</td>\n","      <td>43.3</td>\n","      <td>0.183</td>\n","      <td>33</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1</td>\n","      <td>115</td>\n","      <td>70</td>\n","      <td>30</td>\n","      <td>96</td>\n","      <td>34.6</td>\n","      <td>0.529</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>3</td>\n","      <td>126</td>\n","      <td>88</td>\n","      <td>41</td>\n","      <td>235</td>\n","      <td>39.3</td>\n","      <td>0.704</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>8</td>\n","      <td>99</td>\n","      <td>84</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>35.4</td>\n","      <td>0.388</td>\n","      <td>50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>7</td>\n","      <td>196</td>\n","      <td>90</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>39.8</td>\n","      <td>0.451</td>\n","      <td>41</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>9</td>\n","      <td>119</td>\n","      <td>80</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>29.0</td>\n","      <td>0.263</td>\n","      <td>29</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11</td>\n","      <td>143</td>\n","      <td>94</td>\n","      <td>33</td>\n","      <td>146</td>\n","      <td>36.6</td>\n","      <td>0.254</td>\n","      <td>51</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>10</td>\n","      <td>125</td>\n","      <td>70</td>\n","      <td>26</td>\n","      <td>115</td>\n","      <td>31.1</td>\n","      <td>0.205</td>\n","      <td>41</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>7</td>\n","      <td>147</td>\n","      <td>76</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>39.4</td>\n","      <td>0.257</td>\n","      <td>43</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>1</td>\n","      <td>97</td>\n","      <td>66</td>\n","      <td>15</td>\n","      <td>140</td>\n","      <td>23.2</td>\n","      <td>0.487</td>\n","      <td>22</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>13</td>\n","      <td>145</td>\n","      <td>82</td>\n","      <td>19</td>\n","      <td>110</td>\n","      <td>22.2</td>\n","      <td>0.245</td>\n","      <td>57</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>5</td>\n","      <td>117</td>\n","      <td>92</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>34.1</td>\n","      <td>0.337</td>\n","      <td>38</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>738</th>\n","      <td>2</td>\n","      <td>99</td>\n","      <td>60</td>\n","      <td>17</td>\n","      <td>160</td>\n","      <td>36.6</td>\n","      <td>0.453</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>739</th>\n","      <td>1</td>\n","      <td>102</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>39.5</td>\n","      <td>0.293</td>\n","      <td>42</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>740</th>\n","      <td>11</td>\n","      <td>120</td>\n","      <td>80</td>\n","      <td>37</td>\n","      <td>150</td>\n","      <td>42.3</td>\n","      <td>0.785</td>\n","      <td>48</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>741</th>\n","      <td>3</td>\n","      <td>102</td>\n","      <td>44</td>\n","      <td>20</td>\n","      <td>94</td>\n","      <td>30.8</td>\n","      <td>0.400</td>\n","      <td>26</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>742</th>\n","      <td>1</td>\n","      <td>109</td>\n","      <td>58</td>\n","      <td>18</td>\n","      <td>116</td>\n","      <td>28.5</td>\n","      <td>0.219</td>\n","      <td>22</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>743</th>\n","      <td>9</td>\n","      <td>140</td>\n","      <td>94</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>32.7</td>\n","      <td>0.734</td>\n","      <td>45</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>744</th>\n","      <td>13</td>\n","      <td>153</td>\n","      <td>88</td>\n","      <td>37</td>\n","      <td>140</td>\n","      <td>40.6</td>\n","      <td>1.174</td>\n","      <td>39</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>745</th>\n","      <td>12</td>\n","      <td>100</td>\n","      <td>84</td>\n","      <td>33</td>\n","      <td>105</td>\n","      <td>30.0</td>\n","      <td>0.488</td>\n","      <td>46</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>746</th>\n","      <td>1</td>\n","      <td>147</td>\n","      <td>94</td>\n","      <td>41</td>\n","      <td>0</td>\n","      <td>49.3</td>\n","      <td>0.358</td>\n","      <td>27</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>747</th>\n","      <td>1</td>\n","      <td>81</td>\n","      <td>74</td>\n","      <td>41</td>\n","      <td>57</td>\n","      <td>46.3</td>\n","      <td>1.096</td>\n","      <td>32</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>748</th>\n","      <td>3</td>\n","      <td>187</td>\n","      <td>70</td>\n","      <td>22</td>\n","      <td>200</td>\n","      <td>36.4</td>\n","      <td>0.408</td>\n","      <td>36</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>749</th>\n","      <td>6</td>\n","      <td>162</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24.3</td>\n","      <td>0.178</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>750</th>\n","      <td>4</td>\n","      <td>136</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>31.2</td>\n","      <td>1.182</td>\n","      <td>22</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>751</th>\n","      <td>1</td>\n","      <td>121</td>\n","      <td>78</td>\n","      <td>39</td>\n","      <td>74</td>\n","      <td>39.0</td>\n","      <td>0.261</td>\n","      <td>28</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>752</th>\n","      <td>3</td>\n","      <td>108</td>\n","      <td>62</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>26.0</td>\n","      <td>0.223</td>\n","      <td>25</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>753</th>\n","      <td>0</td>\n","      <td>181</td>\n","      <td>88</td>\n","      <td>44</td>\n","      <td>510</td>\n","      <td>43.3</td>\n","      <td>0.222</td>\n","      <td>26</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>754</th>\n","      <td>8</td>\n","      <td>154</td>\n","      <td>78</td>\n","      <td>32</td>\n","      <td>0</td>\n","      <td>32.4</td>\n","      <td>0.443</td>\n","      <td>45</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>755</th>\n","      <td>1</td>\n","      <td>128</td>\n","      <td>88</td>\n","      <td>39</td>\n","      <td>110</td>\n","      <td>36.5</td>\n","      <td>1.057</td>\n","      <td>37</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>756</th>\n","      <td>7</td>\n","      <td>137</td>\n","      <td>90</td>\n","      <td>41</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>0.391</td>\n","      <td>39</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>757</th>\n","      <td>0</td>\n","      <td>123</td>\n","      <td>72</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>36.3</td>\n","      <td>0.258</td>\n","      <td>52</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>758</th>\n","      <td>1</td>\n","      <td>106</td>\n","      <td>76</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>37.5</td>\n","      <td>0.197</td>\n","      <td>26</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>759</th>\n","      <td>6</td>\n","      <td>190</td>\n","      <td>92</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>35.5</td>\n","      <td>0.278</td>\n","      <td>66</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>760</th>\n","      <td>2</td>\n","      <td>88</td>\n","      <td>58</td>\n","      <td>26</td>\n","      <td>16</td>\n","      <td>28.4</td>\n","      <td>0.766</td>\n","      <td>22</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>761</th>\n","      <td>9</td>\n","      <td>170</td>\n","      <td>74</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>44.0</td>\n","      <td>0.403</td>\n","      <td>43</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>762</th>\n","      <td>9</td>\n","      <td>89</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22.5</td>\n","      <td>0.142</td>\n","      <td>33</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>763</th>\n","      <td>10</td>\n","      <td>101</td>\n","      <td>76</td>\n","      <td>48</td>\n","      <td>180</td>\n","      <td>32.9</td>\n","      <td>0.171</td>\n","      <td>63</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>764</th>\n","      <td>2</td>\n","      <td>122</td>\n","      <td>70</td>\n","      <td>27</td>\n","      <td>0</td>\n","      <td>36.8</td>\n","      <td>0.340</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>765</th>\n","      <td>5</td>\n","      <td>121</td>\n","      <td>72</td>\n","      <td>23</td>\n","      <td>112</td>\n","      <td>26.2</td>\n","      <td>0.245</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>766</th>\n","      <td>1</td>\n","      <td>126</td>\n","      <td>60</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.1</td>\n","      <td>0.349</td>\n","      <td>47</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>767</th>\n","      <td>1</td>\n","      <td>93</td>\n","      <td>70</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>30.4</td>\n","      <td>0.315</td>\n","      <td>23</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>768 rows × 9 columns</p>\n","</div>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"faE6yJgKH_hN"},"source":["We will review how to use the following metrics:\n","* [CLAS-1] Classification Accuracy\n","* [CLAS-2] Logarithmic Loss\n","* [CLAS-3] Area Under ROC Curve\n","* [CLAS-4] Confusion Matrix\n","* [CLAS-5] Classification Report"]},{"cell_type":"markdown","metadata":{"id":"MrMybQseH_hO"},"source":["## [CLAS-1] Classification Accuracy"]},{"cell_type":"markdown","metadata":{"id":"24UleZkhH_hO"},"source":["Classification accuracy is **the number of correct predictions made as a ratio of all predictions made**. \n","\n","This is the most common evaluation metric for classification problems, and it is also often the most misused."]},{"cell_type":"markdown","metadata":{"id":"8IHgnXlpPQY1"},"source":["Below is an example of calculating classification accuracy."]},{"cell_type":"code","metadata":{"id":"QIoQ8VxaH_hP","executionInfo":{"status":"ok","timestamp":1617856587593,"user_tz":-120,"elapsed":2134,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","#\n","from sklearn.linear_model import LogisticRegression"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsvmTIQNP8Oz","executionInfo":{"status":"ok","timestamp":1617856587603,"user_tz":-120,"elapsed":2140,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwvS5XSLaxOc","executionInfo":{"status":"ok","timestamp":1617856587604,"user_tz":-120,"elapsed":2138,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["seed = 7"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZqctOAhTcTiJ"},"source":["We do k-fold CV."]},{"cell_type":"code","metadata":{"id":"8gx6xhKflHat","executionInfo":{"status":"ok","timestamp":1617856635839,"user_tz":-120,"elapsed":567,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["kfold = KFold(n_splits=10, random_state=seed)\n","model = LogisticRegression()"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"trt_zhuJH_hT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856637404,"user_tz":-120,"elapsed":928,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"0eb6dc67-f598-4e0e-b652-eb76c4df6f1d"},"source":["# Cross Validation Classification Accuracy\n","scoring = 'accuracy'                                             # <--- \n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Accuracy: 0.770 (0.048)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G7UZCPMcH_hW"},"source":["You can see that the accuracy ratio is reported: we built a model that is approximately 78% accurate."]},{"cell_type":"markdown","metadata":{"id":"UrZFXTsdckgO"},"source":["## <font color='red'>Exercise 1</font>"]},{"cell_type":"markdown","metadata":{"id":"TX5jWvreckGI"},"source":["Measure the time it takes to run the previous cell, by running k-fold CV with different k's, and compare timing and accuracies obtained."]},{"cell_type":"markdown","metadata":{"id":"NlR6mU7wcne2"},"source":["## <font color='green'>Solution 1</font>"]},{"cell_type":"code","metadata":{"id":"k7KKhaGCcmrq","executionInfo":{"status":"ok","timestamp":1617856587605,"user_tz":-120,"elapsed":2096,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["# enter your code here"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BKNM45TQH_hX"},"source":["## [CLAS-2] Logarithmic Loss (aka \"logloss\")"]},{"cell_type":"markdown","metadata":{"id":"_jOB8f1-H_hX"},"source":["Logarithmic loss (or logloss) is **a performance metric for evaluating the predictions of probabilities of membership to a given class**."]},{"cell_type":"markdown","metadata":{"id":"jlQw2gNbjO32"},"source":["Below is an example of calculating logloss for Logistic regression predictions on the Pima Indians onset of diabetes dataset.\n"]},{"cell_type":"code","metadata":{"id":"i2eytcFcH_hY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856587606,"user_tz":-120,"elapsed":2090,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"4d98ba51-29da-4e14-8e49-9277c6bf3b18"},"source":["# Cross Validation Classification LogLoss\n","scoring = 'neg_log_loss'                      #<---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Logloss: -0.493 (0.047)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"feW0tkQDH_hb"},"source":["Smaller logloss is better with 0 representing a perfect logloss. The\n","measure is inverted to be ascending when using the `cross_val_score()` function (see the documentation)."]},{"cell_type":"markdown","metadata":{"id":"dXrSnEVEH_hb"},"source":["## [CLAS-3] Area Under ROC Curve"]},{"cell_type":"markdown","metadata":{"id":"5W0vSUv1H_hc"},"source":["Area under ROC Curve (or AUC for short) is **a performance metric for binary classification problems**. \n","\n","ROC can be broken down into **sensitivity** and **specificity**. A binary classification problem is really a trade-off between sensitivity and specificity.\n","* Sensitivity is the true positive rate (TPR) also called the Recall. It is the number of instances from the positive (first) class that actually predicted correctly.\n","* Specificity is also called the true negative rate (TNR). It is the number of instances from the\n","negative (second) class that were actually predicted correctly. \n","\n","The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random. \n","\n","The example below provides a demonstration of calculating AUC."]},{"cell_type":"code","metadata":{"id":"Ti20zvTkH_hc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856587607,"user_tz":-120,"elapsed":2085,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"1ee82544-6283-43a6-ca66-912a2c9c681b"},"source":["# Cross Validation Classification ROC AUC\n","scoring = 'roc_auc'\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["AUC: 0.824 (0.041)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FxPGLPc6H_hf"},"source":["You can see the AUC is relatively close to 1 and greater than 0.5, suggesting some skills in the predictions."]},{"cell_type":"markdown","metadata":{"id":"DFGrZupoH_hg"},"source":["## [CLAS-4] Confusion Matrix"]},{"cell_type":"markdown","metadata":{"id":"AAU4XTpgH_hg"},"source":["The confusion matrix is **a handy (and more informative) presentation of the accuracy of a model with two or more classes**. \n","\n","The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a ML algorithm. See the frontal lectures for some examples.\n","\n","Below is an example of calculating a confusion matrix for a set of predictions by a Logistic Regression on the Pima Indians onset of diabetes dataset."]},{"cell_type":"markdown","metadata":{"id":"PUQWoyvGC6eC"},"source":["There are (at least) 2 different ways to do so\n","*   NOTE: to compare these two approaches and avoid to do mistakes, we need to re-execute (or just write again for clarity) some cells above - unnecessary if you do just one method, of course..\n"]},{"cell_type":"markdown","metadata":{"id":"Cq5wI17fIGjX"},"source":["### First method"]},{"cell_type":"markdown","metadata":{"id":"FGDPAwL8C-a1"},"source":["The first is not to rely on `cross_val_score` at all: there is no option to have a confusion matrix as scoring function in its call after having done the k-fold CV, so one way is not to do CV at all,  opt for a static splitting and validation, then use `confusion_matrix` directly."]},{"cell_type":"code","metadata":{"id":"KzVNaWxbDvaA","executionInfo":{"status":"ok","timestamp":1617856587607,"user_tz":-120,"elapsed":2042,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.model_selection import train_test_split      # <---\n","from sklearn.metrics import confusion_matrix              # <---"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ogz6JAG4C-FJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856587608,"user_tz":-120,"elapsed":2038,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"9b8ef002-db35-4384-a51b-9759b9853a78"},"source":["test_size = 0.33\n","\n","# Cross Validation Classification Confusion Matrix\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n","model = LogisticRegression(solver='lbfgs', max_iter=500)\n","model.fit(X_train, Y_train)\n","predicted = model.predict(X_test)\n","matrix1 = confusion_matrix(Y_test, predicted)              # <---\n","print(matrix1)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[[142  20]\n"," [ 34  58]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-oYYi-T6H_hm"},"source":["Although the array is printed without headings, you can see that the majority of the predictions fall on the diagonal line of the matrix (which are correct predictions)."]},{"cell_type":"markdown","metadata":{"id":"WXzz84ZbIJWf"},"source":["### Second method"]},{"cell_type":"markdown","metadata":{"id":"ipuz93TsEMaz"},"source":["The second is keep doing k-fold CV, but to drop the use of `cross_val_score` in favour of `cross_val_predict`.\n"]},{"cell_type":"code","metadata":{"id":"MuWMtsTzH_hh","executionInfo":{"status":"ok","timestamp":1617856587608,"user_tz":-120,"elapsed":1992,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.model_selection import KFold                 # <---\n","from sklearn.model_selection import cross_val_predict     # <---\n","from sklearn.metrics import confusion_matrix              # <---"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xtwfoCzBgRn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856587804,"user_tz":-120,"elapsed":2183,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"73b1ef39-a332-4a79-a468-a827ab68f222"},"source":["kfold = KFold(n_splits=4, random_state=seed)\n","model = LogisticRegression(solver='lbfgs', max_iter=300)\n","\n","predicted = cross_val_predict(model, X, Y, cv=kfold)    # <--- NOTE: no 'scoring'\n","matrix2 = confusion_matrix(Y, predicted)\n","print(matrix2)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[[439  61]\n"," [117 151]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0OfnCOyaIlK6"},"source":["Same as above: the majority of the predictions fall on the diagonal line of the matrix. Good."]},{"cell_type":"markdown","metadata":{"id":"vbchSWFwHquU"},"source":["Let's make a couple of plots. We discuss later."]},{"cell_type":"code","metadata":{"id":"XoqjW7WYHK5-","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"status":"ok","timestamp":1617856588426,"user_tz":-120,"elapsed":2803,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"1963e4b8-7ded-42fc-cd88-de6c5556fd35"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","#\n","df_cm = pd.DataFrame(matrix1)\n","plt.figure(figsize = (10,7))\n","sn.heatmap(df_cm, annot=True, cmap=\"YlOrRd\", fmt=\"d\")"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f5c14117a90>"]},"metadata":{"tags":[]},"execution_count":14},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"AauTmkozHe_Y","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"status":"ok","timestamp":1617856588700,"user_tz":-120,"elapsed":3074,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"1e07a317-666c-488a-f22d-2d850a220782"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","#\n","df_cm = pd.DataFrame(matrix2)\n","plt.figure(figsize = (10,7))\n","sn.heatmap(df_cm, annot=True, cmap=\"YlOrRd\", fmt=\"d\")"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f5c14934fd0>"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAiYAAAGfCAYAAABr4xlmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAGJ9JREFUeJzt3XnUZVV5J+DfWyKDAyJTgAJlHgwunEBXXA7BIYhp0GhWdBE1hnTZLrWNI4JJs4gBh6TRdkqsCCtqO8RZGg2JoradDgpIHBi1QMEqUVQGURRFdv9R1+oCqfpKvN937971PGud5b3nnHvP3n981svv3efcaq0FAGAeLJv1AAAAfklhAgDMDYUJADA3FCYAwNxQmAAAc0NhAgDMDYUJADA3FCYAwNxQmAAAc2OLxb7ASXWAR8vCDJzY3jPrIcBm7MG1lFeb5r+1J7bLlnTstycxAQDmxqInJgDA4hopZRhpLgBA5yQmANC5kVIGhQkAdG6kwmSkuQAAnZOYAEDnRkoZFCYA0LmRCpOR5gIAdE5iAgCdm+mjWqdMYQIAnRup/THSXACAzklMAKBzI6UMChMA6NxIhclIcwEAOicxAYDOjZQyKEwAoHMjFSYjzQUA6JzEBAA6N1LKoDABgM6NVJiMNBcAoHMSEwDo3Egpg8IEADo3UmEy0lwAgM5JTACgcyOlDAoTAOjcSIXJSHMBADonMQGAztWsBzBFChMA6NxI7Y+R5gIAdE5iAgCdGyllUJgAQOcUJgDA3BipMBlpLgBA5yQmANC5kVKGkeYCAJulZVPcNkVV3aWq/qOqzpy836uqvlBVq6rqn6pqy8n+rSbvV02O77kpcwEA+HW8MMkl671/bZLXt9b2TXJdkmMn+49Nct1k/+sn522UwgQAOreUiUlV7Z7kiUnePnlfSQ5P8sHJKe9I8qTJ66Mn7zM5/pjJ+RtkjQkAdG6JU4Y3JHl5kntO3u+Q5PrW2i2T96uTLJ+8Xp7kW0nSWrulqm6YnP/9DX25xAQAWKeqVlTV+ettK9Y79vtJrmmtfXGxri8xAYDOTTNlaK2tTLJyA4cfnuSoqjoyydZJtk3yP5JsV1VbTFKT3ZOsmZy/JskeSVZX1RZJ7pXkBxu7vsQEADq3VGtMWmvHt9Z2b63tmeRpST7dWjsmyWeSPHVy2rOSfGzy+ozJ+0yOf7q11haaCwDAb+K4JC+uqlVZu4bktMn+05LsMNn/4iSvWOiLtHIAoHMbvc1lkbTWPpvks5PXVyQ57A7O+WmSP/x1vldhAgCdG6n9MdJcAIDOSUwAoHMjpQwKEwDo3EiFyUhzAQA6JzEBgM5t/Ndn+qIwAYDOLauNPrOsK1o5AMDckJgAQOe0cgCAuTFQXaKVAwDMD4kJAHSuBlr8qjABgM6NtMZEKwcAmBsSEwDo3EiJicIEADrnAWsAAItAYgIAnRuok6MwAYDejbTGRCsHAJgbEhMA6NxIiYnCBAA6N9KTX7VyAIC5ITEBgM4t08oBAObFSGtMtHIAgLkhMQGAzlXGWfyqMAGAzmnlAAAsAokJAHRupMREYQIAnVvmAWsAANMnMQGAzmnlAABzY6C6RCsHAJgfEhMA6NxIvy6sMAGAzo20xkQrBwCYGxITAOjcsoESE4UJAHRupDUmWjkAwNyQmABA5wbq5ChMAKB3I92VozABgM5ZYwIAsAgkJgDQObcLM5Ratiz/+fwP5cY13817/9N/yVFvPzm7PuTgVFV+8LVv5KN/cnx+/uObcq/77JajTj8ld99p+/zk2uvz4T9+WW5c891ZDx+698Mf/jh/8Rf/kK997Vupqpxyyop85zvX5s1v/lAuv/zb+cAHXpX733/vWQ+TOTbSGhOtHPLQFz4z37/k8nXvz3rRKXnbA47O3x9yVG646uoc9vxjkiSP+9vj8pV3fjR/f8hR+d9/9dY85tUvmdWQYSgnn/zOPOIRh+Sss/57Pvax12SffZZn//33yJve9KIceuiBsx4eLKkFC5OqOrCqjquqN06246rqoKUYHIvvnst/K/s98dG54O0fXLfvZzf+eN3ru26zdTJZU7XT/fbJNz79+STJNz/z+Rx49GOWdKwwohtvvCnnnXdpnvrURydJttxyi2y77d2zzz7Ls/feu812cHSjanrbrG20MKmq45K8L2tvkT53slWS91bVKxZ/eCy2I95wQj718r9Ju/XW2+w/6vRT8pLv/N/scODe+cKb3pUk+e6XL81Bf/D4JMmBT35cttr2Htlm++2WfMwwktWrr8n2298zxx//tjzpScfnla9cmZtu+umsh0VnKm1q26wtlJgcm+TQ1tprWmv/c7K9Jslhk2N3qKpWVNX5VXX++bl+muNlivZ74qPz42uuzdUXXPQrx8740xNy6m6PyPcvuTwH/9GRSZJ/fenrct9HHZoVF3wkez7qsPxw9Xdy6y9+sdTDhqHccsutufjib+bpT39sPvrRV2ebbbbKypVnzHpYMDMLFSa3JrmjLHHXybE71Fpb2Vp7SGvtIQ+J/6KeV/d5+INywFGH54XfODtPfd+p2evwh+XJ7/qbdcfbrbfmwvd9PAc9ZW1K8qOrr8n7n/KCrHzQk3P2K1+fJLn5hhtnMnYYxS67bJ9ddtk+hxyyb5LkiCMemosv/uZsB0V3RmrlLHRXzp8nObuqvp7kW5N990myb5LnL+bAWHxnn3Bqzj7h1CTJfR91WH7npX+ajzzjZbn3PvfJdZdflSQ54KjD8/1Lr0iSbLPDvfOTa69PWssjjl+R/zj9QzMbO4xip522yy677JArrvh29t57t5xzzoXZZ5/lsx4WnamB7hfeaGHSWjurqvbP2tbNL/9S1iQ5r7Umwx9RVZ70jtdmq23vnqrKd758WT7+3BOTJHs++rA85tUvTlrLlZ87P5943kkzHiyM4S//8ll56Uvfkp///JbsscfOefWrn5NPfvK8vOpV78i11/4wz3nO63LQQffNaacdP+uhwqKr1hZ3octJdcDsV9LAZujE9p5ZDwE2Yw9e0gjj2/e9z9T+rd3tyqtmGr94wBoAdG4e1oZMiwesAQBzQ2ICAL3bXBa/AgDzrwbqfww0FQCgdxITAOhcDbT6VWECAJ3TygEAWAQSEwDonVYOADAvtHIAABaBxAQAOrfZ/LowADD/BlpiopUDAMwPiQkAdG6kxa8KEwDo3UBrTAaqsQCAxVRVW1fVuVX15aq6qKpOmux/d1VdVlUXVtXpVXXXyf6qqjdW1aqq+kpVPWihayhMAKBzVdPbFnBzksNba4ckeUCSI6rqYUneneTAJPdPsk2SP5uc/4Qk+022FUn+bqELaOUAQOeW6nbh1lpL8qPJ27tOttZa+8S6sVSdm2T3ydujk7xz8rnPV9V2VbVra+3qDV1DYgIAbLKquktVfSnJNUk+2Vr7wnrH7prkGUnOmuxanuRb63189WTfBilMAKBztWyKW9WKqjp/vW3F+tdqrf2itfaArE1FDquqg9c7/NYkn2ut/Z87OxetHADoXE3xCWuttZVJVm7CeddX1WeSHJHkwqo6MclOSZ6z3mlrkuyx3vvdJ/s2SGICAGySqtqpqrabvN4myeOSXFpVf5bk95I8vbV263ofOSPJMyd35zwsyQ0bW1+SSEwAoH9LFzPsmuQdVXWXyVXf31o7s6puSXJlknMm6c2HW2t/leQTSY5MsirJTUmevdAFFCYA0Lml+q2c1tpXkjzwDvbfYT0xuRvneb/ONbRyAIC5ITEBgM4t1XNMloLCBAA6N9KP+A00FQCgdxITAOjdUq1+XQIKEwDonFYOAMAikJgAQOfclQMAzI2Blpho5QAA80NiAgCd08oBAObHOHWJVg4AMD8kJgDQuZGeY6IwAYDOjbTGZKAaCwDoncQEADo30nNMFCYA0LmRWjkKEwDo3UALMwaaCgDQO4kJAPROKwcAmBsD9T8GmgoA0DuJCQD0TisHAJgbA/U/BpoKANA7iQkA9E4rBwCYGwMVJlo5AMDckJgAQO8GihkUJgDQO60cAIDpk5gAQO8GihkUJgDQO60cAIDpk5gAQO/GCUwUJgDQPa0cAIDpk5gAQO8GSkwUJgDQu4H6HwNNBQDoncQEAHqnlQMAzIsaqP8x0FQAgN5JTACgd1o5AMDcGKj/MdBUAIDeSUwAoHdaOQDA3BioMNHKAQDmhsQEAHo3UMygMAGA3mnlAABMn8QEAHo3UMygMAGA3mnlAABMn8QEAHo3UMygMAGA3mnlAABMn8QEAHo3UMygMAGA3mnlAABMn8QEAHo3UGKiMAGA3g3U/xhoKgBA7xY9MTnxmpcs9iWAO9BWfWTWQ4DNVu374KW9oFYOADA3Bup/DDQVAKB3EhMA6F1p5QAA82KcukRhAgDdGygxscYEAJgbChMA6F1NcdvYZar2qKrPVNXFVXVRVb3wdsdfUlWtqnacvK+qemNVraqqr1TVgxaailYOAPRu6Vo5tyR5SWvtgqq6Z5IvVtUnW2sXV9UeSR6f5Kr1zn9Ckv0m20OT/N3kfzdIYgIAbJLW2tWttQsmr29MckmS5ZPDr0/y8iRtvY8cneSdba3PJ9muqnbd2DUkJgDQuxnEDFW1Z5IHJvlCVR2dZE1r7ct12/RmeZJvrfd+9WTf1Rv6XoUJAPRuiq2cqlqRZMV6u1a21lbe7px7JPlQkj/P2vbOCVnbxvmNKUwAgHUmRcjKDR2vqrtmbVHy7tbah6vq/kn2SvLLtGT3JBdU1WFJ1iTZY72P7z7Zt0HWmABA75burpxKclqSS1prpyZJa+2rrbWdW2t7ttb2zNp2zYNaa99JckaSZ07uznlYkhtaaxts4yQSEwDo39LdlfPwJM9I8tWq+tJk3wmttU9s4PxPJDkyyaokNyV59kIXUJgAAJuktfZvWSBXmaQmv3zdkjzv17mGwgQAejfOE+kVJgDQPb+VAwAwfRITAOjdQDGDwgQAeqeVAwAwfRITAOjdOIGJwgQAuqeVAwAwfRITAOjcQIGJwgQAujdQZaKVAwDMDYkJAPRunMBEYQIA3Vs2TmWilQMAzA2JCQD0bpzARGECAN1zVw4AwPRJTACgd+MEJgoTAOieVg4AwPRJTACgd+MEJgoTAOieB6wBAEyfxAQAejdOYKIwAYDuuSsHAGD6JCYA0LtxAhOFCQB0TysHAGD6JCYA0LtxAhOFCQB0zwPWAACmT2ICAL0baPGrwgQAejdQYaKVAwDMDYkJAPRuoMREYQIAvatxGiDjzAQA6J7EBAB6N9BzTBQmANC7gdaYaOUAAHNDYgIAvRto8avCBAB6N1ArR2ECAL0baPHrONkPANA9iQkA9M4aEwBgbgy0xmScEgsA6J7EBAB6N1BiojABgN4NtMZknJkAAN2TmABA7wZ6jonCBAB6N9AaE60cAGBuSEwAoHcDLX5VmABA77RyAACmT2ICAL1zVw4AMDcGWmMyzkwAgO5JTACgdwMtflWYAEDvBipMtHIAgLkhMQGA3g2UmChMAKB3y8ZpgIwzEwCgexITAOidVg4AMDcGKky0cgCAuSExAYDeDfRIeoUJAPRuoB/xG6fEAgAWXVWdXlXXVNWFt9v/gqq6tKouqqrXrbf/+KpaVVWXVdXvLfT9EhMA6N3SLn79xyRvTvLO/3/5+t0kRyc5pLV2c1XtPNl/vyRPS/LbSXZL8qmq2r+19osNfbnEBAB6V8umty2gtfa5JNfebvdzk7ymtXbz5JxrJvuPTvK+1trNrbVvJFmV5LCNfb/EZDN2/Cln5bP/fkV2uPfdcua7/iRJ8s+fvixvPv2cXH7lD/KBfzgm9z9wlyTJGf96SU57z3nrPnvZ5d/LR05/Rg7ab+cZjBz6d8Ibzstnz706O2y3Vf7XW9em229690X5wL9cke233SpJ8qJn3T+POnTXXPfDm/PCU87JhV+/Nk967J75b8990CyHzuCqakWSFevtWtlaW7nAx/ZP8oiqOjnJT5O8tLV2XpLlST6/3nmrJ/s2SGGyGfuDIw/OHz/lgTnur/953b79994xbzrlqJz4uk/e5tyjHn9Qjnr8QUnWFiXPO/5jihL4DTz5sXvmmN/fN6849dzb7H/W0fvn2KcccJt9W215l7zwGQfn61fekK9decNSDpNeTLGVMylCFipEbm+LJNsneViSQ5O8v6r2vjPXV5hsxg59wO5ZffVt/09unz13WPBzH//UpXniYw5crGHBZuHQg3fK6u/+eJPOvdvWW+TBv71jrvz2jxZ5VHRr9g9YW53kw621luTcqro1yY5J1iTZY73zdp/s26A7vcakqp59Zz9L3z5x9mV54uMUJrAY3n3mqhz1vH/NCW84Lzfc+LNZDwc21UeT/G6SVNX+SbZM8v0kZyR5WlVtVVV7Jdkvybkb/Jb8ZotfT/oNPkunvnzR1dlm67tm/713nPVQYDhPP3KffPLtR+ajb3pcdrr31nntaV+e9ZDoxbJl09sWUFXvTXJOkgOqanVVHZvk9CR7T24hfl+SZ7W1Lkry/iQXJzkryfM2dkdOskArp6q+sqFDSX5rI59bt3DmbX97TFY885Ebuwwd+fjZl+aJj5WWwGLY8d5br3v9h0fsneee9G8zHA19WbpWTmvt6Rs49McbOP/kJCdv6vcvtMbkt5L8XpLrbre/kvz7hj50m4Uz31vZNnUwzLdbb235509/Le95yx/NeigwpGuu/Ul23n6bJMmn/n1N9rvvvWY8Ilh6CxUmZya5R2vtS7c/UFWfXZQRsWRefOKZOfdLq3Pd9T/JI5/8trzg2N/JdvfcOq96w6dz7fU/yXNe9pEctN9OOe3UpyZJzvvS6uy68z2zx/LtZjxy6N+LX/v5nPfV7+W6H96cRz3zzLzgmN/OuV+9JpdccX2qKst3vltOesGD151/+LM/nh/f9PP8/JZbc/Y5385pf/3I7HufbWc4A+bK7Be/Tk2tXUC7iCQmMBPthqtmPQTYbNW+f72klUJb9cqp/Vtb+5480yrHk18BgLnhOSYA0L1xWjkKEwDo3UBrTLRyAIC5ITEBgN5twq8C90JhAgDd08oBAJg6iQkA9G6gxa8KEwDo3jgNkHFmAgB0T2ICAL3TygEA5sZAhYlWDgAwNyQmANC9cRIThQkA9M6TXwGAuWGNCQDA9ElMAKB74yQmChMA6N1Aa0zGmQkA0D2JCQB0rgZa/KowAYDujVOYaOUAAHNDYgIAvRto8avCBAC6p5UDADB1EhMA6J27cgCAuTHQGpNxZgIAdE9iAgDd08oBAObFQGtMtHIAgLkhMQGA3g20+FVhAgDd08oBAJg6iQkA9G6gxa8KEwDo3jgNkHFmAgB0T2ICAL3TygEA5sZAhYlWDgAwNyQmANC9cXIGhQkA9E4rBwBg+iQmANC9cRIThQkA9G6gH/EbZyYAQPckJgDQu4EWvypMAKB74xQmWjkAwNyQmABA7wZa/KowAYDuaeUAAEydxAQAeueuHABgfozTABlnJgBA9yQmANA7rRwAYG4MdLvwODMBALonMQGA7mnlAADzYqA1Jlo5AMDckJgAQPfGyRkUJgDQO60cAIDpk5gAQPfGyRkUJgDQO60cAIDpU5gAQO+qprcteKl6UVVdVFUXVtV7q2rrqtqrqr5QVauq6p+qass7OxWFCQB0b9kUtw2rquVJ/muSh7TWDk5ylyRPS/LaJK9vre2b5Lokx/4mMwEAeraEiUnWrk/dpqq2SHK3JFcnOTzJByfH35HkSXd2KgoTAGCTtNbWJPnbJFdlbUFyQ5IvJrm+tXbL5LTVSZbf2WsoTACgezW1rapWVNX5620r1l2l6t5Jjk6yV5Ldktw9yRHTnInbhQGgdzW9nKG1tjLJyg0cfmySb7TWvpckVfXhJA9Psl1VbTFJTXZPsubOXl9iAgBsqquSPKyq7lZVleQxSS5O8pkkT52c86wkH7uzF1CYAED3ptfK2ZjW2heydpHrBUm+mrV1xMokxyV5cVWtSrJDktPu7Ey0cgCgd1Ns5SyktXZikhNvt/uKJIdN4/slJgDA3JCYAED3xvmtHIUJAPTOj/gBAEyfxAQAereEi18Xm8IEALqnlQMAMHUSEwDo3UCLXxUmANC9cRog48wEAOiexAQAeqeVAwDMj3EaIOPMBADonsQEAHo3UCunWmuzHgNzrKpWtNZWznocsLnxt8fmSiuHhayY9QBgM+Vvj82SwgQAmBsKEwBgbihMWIgeN8yGvz02Sxa/AgBzQ2ICAMwNhQl3qKqOqKrLqmpVVb1i1uOBzUVVnV5V11TVhbMeC8yCwoRfUVV3SfKWJE9Icr8kT6+q+812VLDZ+MckR8x6EDArChPuyGFJVrXWrmit/SzJ+5IcPeMxwWahtfa5JNfOehwwKwoT7sjyJN9a7/3qyT4AWFQKEwBgbihMuCNrkuyx3vvdJ/sAYFEpTLgj5yXZr6r2qqotkzwtyRkzHhMAmwGFCb+itXZLkucn+ZcklyR5f2vtotmOCjYPVfXeJOckOaCqVlfVsbMeEywlT34FAOaGxAQAmBsKEwBgbihMAIC5oTABAOaGwgQAmBsKEwBgbihMAIC5oTABAObG/wPIXWQWz04zugAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 720x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"iBkJ2ZEq61vx"},"source":["## <font color='red'>Exercise 2</font>"]},{"cell_type":"markdown","metadata":{"id":"kh6KgPD4Hj0Q"},"source":["The 2 matrices are not the same, though, aren't they? So: are the 2 results, content-wise, the same? or comparable?\n"]},{"cell_type":"markdown","metadata":{"id":"4UKxoLEZ637j"},"source":["## <font color='green'>Solution 2</font>"]},{"cell_type":"code","metadata":{"id":"FDFM48xA7IkV","executionInfo":{"status":"ok","timestamp":1617856588702,"user_tz":-120,"elapsed":3071,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["# write your code here"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UX04UJW1H_hn"},"source":["## [CLAS-5] Classification Report"]},{"cell_type":"markdown","metadata":{"id":"qmB2Dqb_OY65"},"source":["There is also **a convenience report provided by the scikit-learn library** when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures. The `classification report()` function displays the precision, recall, F1-score and support for each class. \n","\n","The example below demonstrates the report on the binary classification problem."]},{"cell_type":"code","metadata":{"id":"ds9M8G2NOb1q","executionInfo":{"status":"ok","timestamp":1617856588704,"user_tz":-120,"elapsed":3039,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.metrics import classification_report               # <---"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"x23ukIh1Ob1v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856588705,"user_tz":-120,"elapsed":3033,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"6395a2a6-ca1d-4c05-c84e-ba45b8d4782f"},"source":["test_size = 0.33\n","seed = 7\n","\n","# Cross Validation Classification Report\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n","    random_state=seed)\n","model = LogisticRegression()\n","model.fit(X_train, Y_train)\n","predicted = model.predict(X_test)\n","report = classification_report(Y_test, predicted)               # <---\n","print(report)                  "],"execution_count":20,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.77      0.87      0.82       162\n","         1.0       0.71      0.55      0.62        92\n","\n","   micro avg       0.76      0.76      0.76       254\n","   macro avg       0.74      0.71      0.72       254\n","weighted avg       0.75      0.76      0.75       254\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fSm3BxC8OiaX"},"source":["You can see good prediction and recall for the algorithm."]},{"cell_type":"markdown","metadata":{"id":"d9hZrzyMH_hu"},"source":["# REGRESSION Metrics"]},{"cell_type":"markdown","metadata":{"id":"9tUWR8cj979e"},"source":["In the regression examples, we will use the Boston house price dataset, which you can find (original source) [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data), and for your convenience it is already in the github repo of the course."]},{"cell_type":"markdown","metadata":{"id":"we42m9SZ8DIe"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"F5BbQyy-8DIg","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1617857249799,"user_tz":-120,"elapsed":1052,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"fbbb47bc-8192-4dd3-e730-9147d245d52b"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML2021Bas/main/datasets/housing.data.csv'\n","\n","names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n","data = pd.read_csv(url, delim_whitespace=True, names=names)\n","data\n","\n","#array = dataframe.values\n","#X = array[:,0:13]\n","#Y = array[:,13]\n"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n","0     0.00632  18.0   2.31     0  0.538  6.575   65.2  4.0900    1  296.0   \n","1     0.02731   0.0   7.07     0  0.469  6.421   78.9  4.9671    2  242.0   \n","2     0.02729   0.0   7.07     0  0.469  7.185   61.1  4.9671    2  242.0   \n","3     0.03237   0.0   2.18     0  0.458  6.998   45.8  6.0622    3  222.0   \n","4     0.06905   0.0   2.18     0  0.458  7.147   54.2  6.0622    3  222.0   \n","5     0.02985   0.0   2.18     0  0.458  6.430   58.7  6.0622    3  222.0   \n","6     0.08829  12.5   7.87     0  0.524  6.012   66.6  5.5605    5  311.0   \n","7     0.14455  12.5   7.87     0  0.524  6.172   96.1  5.9505    5  311.0   \n","8     0.21124  12.5   7.87     0  0.524  5.631  100.0  6.0821    5  311.0   \n","9     0.17004  12.5   7.87     0  0.524  6.004   85.9  6.5921    5  311.0   \n","10    0.22489  12.5   7.87     0  0.524  6.377   94.3  6.3467    5  311.0   \n","11    0.11747  12.5   7.87     0  0.524  6.009   82.9  6.2267    5  311.0   \n","12    0.09378  12.5   7.87     0  0.524  5.889   39.0  5.4509    5  311.0   \n","13    0.62976   0.0   8.14     0  0.538  5.949   61.8  4.7075    4  307.0   \n","14    0.63796   0.0   8.14     0  0.538  6.096   84.5  4.4619    4  307.0   \n","15    0.62739   0.0   8.14     0  0.538  5.834   56.5  4.4986    4  307.0   \n","16    1.05393   0.0   8.14     0  0.538  5.935   29.3  4.4986    4  307.0   \n","17    0.78420   0.0   8.14     0  0.538  5.990   81.7  4.2579    4  307.0   \n","18    0.80271   0.0   8.14     0  0.538  5.456   36.6  3.7965    4  307.0   \n","19    0.72580   0.0   8.14     0  0.538  5.727   69.5  3.7965    4  307.0   \n","20    1.25179   0.0   8.14     0  0.538  5.570   98.1  3.7979    4  307.0   \n","21    0.85204   0.0   8.14     0  0.538  5.965   89.2  4.0123    4  307.0   \n","22    1.23247   0.0   8.14     0  0.538  6.142   91.7  3.9769    4  307.0   \n","23    0.98843   0.0   8.14     0  0.538  5.813  100.0  4.0952    4  307.0   \n","24    0.75026   0.0   8.14     0  0.538  5.924   94.1  4.3996    4  307.0   \n","25    0.84054   0.0   8.14     0  0.538  5.599   85.7  4.4546    4  307.0   \n","26    0.67191   0.0   8.14     0  0.538  5.813   90.3  4.6820    4  307.0   \n","27    0.95577   0.0   8.14     0  0.538  6.047   88.8  4.4534    4  307.0   \n","28    0.77299   0.0   8.14     0  0.538  6.495   94.4  4.4547    4  307.0   \n","29    1.00245   0.0   8.14     0  0.538  6.674   87.3  4.2390    4  307.0   \n","..        ...   ...    ...   ...    ...    ...    ...     ...  ...    ...   \n","476   4.87141   0.0  18.10     0  0.614  6.484   93.6  2.3053   24  666.0   \n","477  15.02340   0.0  18.10     0  0.614  5.304   97.3  2.1007   24  666.0   \n","478  10.23300   0.0  18.10     0  0.614  6.185   96.7  2.1705   24  666.0   \n","479  14.33370   0.0  18.10     0  0.614  6.229   88.0  1.9512   24  666.0   \n","480   5.82401   0.0  18.10     0  0.532  6.242   64.7  3.4242   24  666.0   \n","481   5.70818   0.0  18.10     0  0.532  6.750   74.9  3.3317   24  666.0   \n","482   5.73116   0.0  18.10     0  0.532  7.061   77.0  3.4106   24  666.0   \n","483   2.81838   0.0  18.10     0  0.532  5.762   40.3  4.0983   24  666.0   \n","484   2.37857   0.0  18.10     0  0.583  5.871   41.9  3.7240   24  666.0   \n","485   3.67367   0.0  18.10     0  0.583  6.312   51.9  3.9917   24  666.0   \n","486   5.69175   0.0  18.10     0  0.583  6.114   79.8  3.5459   24  666.0   \n","487   4.83567   0.0  18.10     0  0.583  5.905   53.2  3.1523   24  666.0   \n","488   0.15086   0.0  27.74     0  0.609  5.454   92.7  1.8209    4  711.0   \n","489   0.18337   0.0  27.74     0  0.609  5.414   98.3  1.7554    4  711.0   \n","490   0.20746   0.0  27.74     0  0.609  5.093   98.0  1.8226    4  711.0   \n","491   0.10574   0.0  27.74     0  0.609  5.983   98.8  1.8681    4  711.0   \n","492   0.11132   0.0  27.74     0  0.609  5.983   83.5  2.1099    4  711.0   \n","493   0.17331   0.0   9.69     0  0.585  5.707   54.0  2.3817    6  391.0   \n","494   0.27957   0.0   9.69     0  0.585  5.926   42.6  2.3817    6  391.0   \n","495   0.17899   0.0   9.69     0  0.585  5.670   28.8  2.7986    6  391.0   \n","496   0.28960   0.0   9.69     0  0.585  5.390   72.9  2.7986    6  391.0   \n","497   0.26838   0.0   9.69     0  0.585  5.794   70.6  2.8927    6  391.0   \n","498   0.23912   0.0   9.69     0  0.585  6.019   65.3  2.4091    6  391.0   \n","499   0.17783   0.0   9.69     0  0.585  5.569   73.5  2.3999    6  391.0   \n","500   0.22438   0.0   9.69     0  0.585  6.027   79.7  2.4982    6  391.0   \n","501   0.06263   0.0  11.93     0  0.573  6.593   69.1  2.4786    1  273.0   \n","502   0.04527   0.0  11.93     0  0.573  6.120   76.7  2.2875    1  273.0   \n","503   0.06076   0.0  11.93     0  0.573  6.976   91.0  2.1675    1  273.0   \n","504   0.10959   0.0  11.93     0  0.573  6.794   89.3  2.3889    1  273.0   \n","505   0.04741   0.0  11.93     0  0.573  6.030   80.8  2.5050    1  273.0   \n","\n","     PTRATIO       B  LSTAT  MEDV  \n","0       15.3  396.90   4.98  24.0  \n","1       17.8  396.90   9.14  21.6  \n","2       17.8  392.83   4.03  34.7  \n","3       18.7  394.63   2.94  33.4  \n","4       18.7  396.90   5.33  36.2  \n","5       18.7  394.12   5.21  28.7  \n","6       15.2  395.60  12.43  22.9  \n","7       15.2  396.90  19.15  27.1  \n","8       15.2  386.63  29.93  16.5  \n","9       15.2  386.71  17.10  18.9  \n","10      15.2  392.52  20.45  15.0  \n","11      15.2  396.90  13.27  18.9  \n","12      15.2  390.50  15.71  21.7  \n","13      21.0  396.90   8.26  20.4  \n","14      21.0  380.02  10.26  18.2  \n","15      21.0  395.62   8.47  19.9  \n","16      21.0  386.85   6.58  23.1  \n","17      21.0  386.75  14.67  17.5  \n","18      21.0  288.99  11.69  20.2  \n","19      21.0  390.95  11.28  18.2  \n","20      21.0  376.57  21.02  13.6  \n","21      21.0  392.53  13.83  19.6  \n","22      21.0  396.90  18.72  15.2  \n","23      21.0  394.54  19.88  14.5  \n","24      21.0  394.33  16.30  15.6  \n","25      21.0  303.42  16.51  13.9  \n","26      21.0  376.88  14.81  16.6  \n","27      21.0  306.38  17.28  14.8  \n","28      21.0  387.94  12.80  18.4  \n","29      21.0  380.23  11.98  21.0  \n","..       ...     ...    ...   ...  \n","476     20.2  396.21  18.68  16.7  \n","477     20.2  349.48  24.91  12.0  \n","478     20.2  379.70  18.03  14.6  \n","479     20.2  383.32  13.11  21.4  \n","480     20.2  396.90  10.74  23.0  \n","481     20.2  393.07   7.74  23.7  \n","482     20.2  395.28   7.01  25.0  \n","483     20.2  392.92  10.42  21.8  \n","484     20.2  370.73  13.34  20.6  \n","485     20.2  388.62  10.58  21.2  \n","486     20.2  392.68  14.98  19.1  \n","487     20.2  388.22  11.45  20.6  \n","488     20.1  395.09  18.06  15.2  \n","489     20.1  344.05  23.97   7.0  \n","490     20.1  318.43  29.68   8.1  \n","491     20.1  390.11  18.07  13.6  \n","492     20.1  396.90  13.35  20.1  \n","493     19.2  396.90  12.01  21.8  \n","494     19.2  396.90  13.59  24.5  \n","495     19.2  393.29  17.60  23.1  \n","496     19.2  396.90  21.14  19.7  \n","497     19.2  396.90  14.10  18.3  \n","498     19.2  396.90  12.92  21.2  \n","499     19.2  395.77  15.10  17.5  \n","500     19.2  396.90  14.33  16.8  \n","501     21.0  391.99   9.67  22.4  \n","502     21.0  396.90   9.08  20.6  \n","503     21.0  396.90   5.64  23.9  \n","504     21.0  393.45   6.48  22.0  \n","505     21.0  396.90   7.88  11.9  \n","\n","[506 rows x 14 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CRIM</th>\n","      <th>ZN</th>\n","      <th>INDUS</th>\n","      <th>CHAS</th>\n","      <th>NOX</th>\n","      <th>RM</th>\n","      <th>AGE</th>\n","      <th>DIS</th>\n","      <th>RAD</th>\n","      <th>TAX</th>\n","      <th>PTRATIO</th>\n","      <th>B</th>\n","      <th>LSTAT</th>\n","      <th>MEDV</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00632</td>\n","      <td>18.0</td>\n","      <td>2.31</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.575</td>\n","      <td>65.2</td>\n","      <td>4.0900</td>\n","      <td>1</td>\n","      <td>296.0</td>\n","      <td>15.3</td>\n","      <td>396.90</td>\n","      <td>4.98</td>\n","      <td>24.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.02731</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>6.421</td>\n","      <td>78.9</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>396.90</td>\n","      <td>9.14</td>\n","      <td>21.6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.02729</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>7.185</td>\n","      <td>61.1</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242.0</td>\n","      <td>17.8</td>\n","      <td>392.83</td>\n","      <td>4.03</td>\n","      <td>34.7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03237</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>6.998</td>\n","      <td>45.8</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>394.63</td>\n","      <td>2.94</td>\n","      <td>33.4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.06905</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>7.147</td>\n","      <td>54.2</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>396.90</td>\n","      <td>5.33</td>\n","      <td>36.2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.02985</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>6.430</td>\n","      <td>58.7</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222.0</td>\n","      <td>18.7</td>\n","      <td>394.12</td>\n","      <td>5.21</td>\n","      <td>28.7</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.08829</td>\n","      <td>12.5</td>\n","      <td>7.87</td>\n","      <td>0</td>\n","      <td>0.524</td>\n","      <td>6.012</td>\n","      <td>66.6</td>\n","      <td>5.5605</td>\n","      <td>5</td>\n","      <td>311.0</td>\n","      <td>15.2</td>\n","      <td>395.60</td>\n","      <td>12.43</td>\n","      <td>22.9</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.14455</td>\n","      <td>12.5</td>\n","      <td>7.87</td>\n","      <td>0</td>\n","      <td>0.524</td>\n","      <td>6.172</td>\n","      <td>96.1</td>\n","      <td>5.9505</td>\n","      <td>5</td>\n","      <td>311.0</td>\n","      <td>15.2</td>\n","      <td>396.90</td>\n","      <td>19.15</td>\n","      <td>27.1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.21124</td>\n","      <td>12.5</td>\n","      <td>7.87</td>\n","      <td>0</td>\n","      <td>0.524</td>\n","      <td>5.631</td>\n","      <td>100.0</td>\n","      <td>6.0821</td>\n","      <td>5</td>\n","      <td>311.0</td>\n","      <td>15.2</td>\n","      <td>386.63</td>\n","      <td>29.93</td>\n","      <td>16.5</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.17004</td>\n","      <td>12.5</td>\n","      <td>7.87</td>\n","      <td>0</td>\n","      <td>0.524</td>\n","      <td>6.004</td>\n","      <td>85.9</td>\n","      <td>6.5921</td>\n","      <td>5</td>\n","      <td>311.0</td>\n","      <td>15.2</td>\n","      <td>386.71</td>\n","      <td>17.10</td>\n","      <td>18.9</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.22489</td>\n","      <td>12.5</td>\n","      <td>7.87</td>\n","      <td>0</td>\n","      <td>0.524</td>\n","      <td>6.377</td>\n","      <td>94.3</td>\n","      <td>6.3467</td>\n","      <td>5</td>\n","      <td>311.0</td>\n","      <td>15.2</td>\n","      <td>392.52</td>\n","      <td>20.45</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.11747</td>\n","      <td>12.5</td>\n","      <td>7.87</td>\n","      <td>0</td>\n","      <td>0.524</td>\n","      <td>6.009</td>\n","      <td>82.9</td>\n","      <td>6.2267</td>\n","      <td>5</td>\n","      <td>311.0</td>\n","      <td>15.2</td>\n","      <td>396.90</td>\n","      <td>13.27</td>\n","      <td>18.9</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.09378</td>\n","      <td>12.5</td>\n","      <td>7.87</td>\n","      <td>0</td>\n","      <td>0.524</td>\n","      <td>5.889</td>\n","      <td>39.0</td>\n","      <td>5.4509</td>\n","      <td>5</td>\n","      <td>311.0</td>\n","      <td>15.2</td>\n","      <td>390.50</td>\n","      <td>15.71</td>\n","      <td>21.7</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.62976</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.949</td>\n","      <td>61.8</td>\n","      <td>4.7075</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>396.90</td>\n","      <td>8.26</td>\n","      <td>20.4</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.63796</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.096</td>\n","      <td>84.5</td>\n","      <td>4.4619</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>380.02</td>\n","      <td>10.26</td>\n","      <td>18.2</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.62739</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.834</td>\n","      <td>56.5</td>\n","      <td>4.4986</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>395.62</td>\n","      <td>8.47</td>\n","      <td>19.9</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1.05393</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.935</td>\n","      <td>29.3</td>\n","      <td>4.4986</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>386.85</td>\n","      <td>6.58</td>\n","      <td>23.1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.78420</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.990</td>\n","      <td>81.7</td>\n","      <td>4.2579</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>386.75</td>\n","      <td>14.67</td>\n","      <td>17.5</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.80271</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.456</td>\n","      <td>36.6</td>\n","      <td>3.7965</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>288.99</td>\n","      <td>11.69</td>\n","      <td>20.2</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.72580</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.727</td>\n","      <td>69.5</td>\n","      <td>3.7965</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>390.95</td>\n","      <td>11.28</td>\n","      <td>18.2</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1.25179</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.570</td>\n","      <td>98.1</td>\n","      <td>3.7979</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>376.57</td>\n","      <td>21.02</td>\n","      <td>13.6</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0.85204</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.965</td>\n","      <td>89.2</td>\n","      <td>4.0123</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>392.53</td>\n","      <td>13.83</td>\n","      <td>19.6</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1.23247</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.142</td>\n","      <td>91.7</td>\n","      <td>3.9769</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>396.90</td>\n","      <td>18.72</td>\n","      <td>15.2</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0.98843</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.813</td>\n","      <td>100.0</td>\n","      <td>4.0952</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>394.54</td>\n","      <td>19.88</td>\n","      <td>14.5</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0.75026</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.924</td>\n","      <td>94.1</td>\n","      <td>4.3996</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>394.33</td>\n","      <td>16.30</td>\n","      <td>15.6</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0.84054</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.599</td>\n","      <td>85.7</td>\n","      <td>4.4546</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>303.42</td>\n","      <td>16.51</td>\n","      <td>13.9</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0.67191</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>5.813</td>\n","      <td>90.3</td>\n","      <td>4.6820</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>376.88</td>\n","      <td>14.81</td>\n","      <td>16.6</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0.95577</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.047</td>\n","      <td>88.8</td>\n","      <td>4.4534</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>306.38</td>\n","      <td>17.28</td>\n","      <td>14.8</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0.77299</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.495</td>\n","      <td>94.4</td>\n","      <td>4.4547</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>387.94</td>\n","      <td>12.80</td>\n","      <td>18.4</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1.00245</td>\n","      <td>0.0</td>\n","      <td>8.14</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.674</td>\n","      <td>87.3</td>\n","      <td>4.2390</td>\n","      <td>4</td>\n","      <td>307.0</td>\n","      <td>21.0</td>\n","      <td>380.23</td>\n","      <td>11.98</td>\n","      <td>21.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>476</th>\n","      <td>4.87141</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.614</td>\n","      <td>6.484</td>\n","      <td>93.6</td>\n","      <td>2.3053</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>396.21</td>\n","      <td>18.68</td>\n","      <td>16.7</td>\n","    </tr>\n","    <tr>\n","      <th>477</th>\n","      <td>15.02340</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.614</td>\n","      <td>5.304</td>\n","      <td>97.3</td>\n","      <td>2.1007</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>349.48</td>\n","      <td>24.91</td>\n","      <td>12.0</td>\n","    </tr>\n","    <tr>\n","      <th>478</th>\n","      <td>10.23300</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.614</td>\n","      <td>6.185</td>\n","      <td>96.7</td>\n","      <td>2.1705</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>379.70</td>\n","      <td>18.03</td>\n","      <td>14.6</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>14.33370</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.614</td>\n","      <td>6.229</td>\n","      <td>88.0</td>\n","      <td>1.9512</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>383.32</td>\n","      <td>13.11</td>\n","      <td>21.4</td>\n","    </tr>\n","    <tr>\n","      <th>480</th>\n","      <td>5.82401</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.532</td>\n","      <td>6.242</td>\n","      <td>64.7</td>\n","      <td>3.4242</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>396.90</td>\n","      <td>10.74</td>\n","      <td>23.0</td>\n","    </tr>\n","    <tr>\n","      <th>481</th>\n","      <td>5.70818</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.532</td>\n","      <td>6.750</td>\n","      <td>74.9</td>\n","      <td>3.3317</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>393.07</td>\n","      <td>7.74</td>\n","      <td>23.7</td>\n","    </tr>\n","    <tr>\n","      <th>482</th>\n","      <td>5.73116</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.532</td>\n","      <td>7.061</td>\n","      <td>77.0</td>\n","      <td>3.4106</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>395.28</td>\n","      <td>7.01</td>\n","      <td>25.0</td>\n","    </tr>\n","    <tr>\n","      <th>483</th>\n","      <td>2.81838</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.532</td>\n","      <td>5.762</td>\n","      <td>40.3</td>\n","      <td>4.0983</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>392.92</td>\n","      <td>10.42</td>\n","      <td>21.8</td>\n","    </tr>\n","    <tr>\n","      <th>484</th>\n","      <td>2.37857</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.583</td>\n","      <td>5.871</td>\n","      <td>41.9</td>\n","      <td>3.7240</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>370.73</td>\n","      <td>13.34</td>\n","      <td>20.6</td>\n","    </tr>\n","    <tr>\n","      <th>485</th>\n","      <td>3.67367</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.583</td>\n","      <td>6.312</td>\n","      <td>51.9</td>\n","      <td>3.9917</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>388.62</td>\n","      <td>10.58</td>\n","      <td>21.2</td>\n","    </tr>\n","    <tr>\n","      <th>486</th>\n","      <td>5.69175</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.583</td>\n","      <td>6.114</td>\n","      <td>79.8</td>\n","      <td>3.5459</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>392.68</td>\n","      <td>14.98</td>\n","      <td>19.1</td>\n","    </tr>\n","    <tr>\n","      <th>487</th>\n","      <td>4.83567</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0</td>\n","      <td>0.583</td>\n","      <td>5.905</td>\n","      <td>53.2</td>\n","      <td>3.1523</td>\n","      <td>24</td>\n","      <td>666.0</td>\n","      <td>20.2</td>\n","      <td>388.22</td>\n","      <td>11.45</td>\n","      <td>20.6</td>\n","    </tr>\n","    <tr>\n","      <th>488</th>\n","      <td>0.15086</td>\n","      <td>0.0</td>\n","      <td>27.74</td>\n","      <td>0</td>\n","      <td>0.609</td>\n","      <td>5.454</td>\n","      <td>92.7</td>\n","      <td>1.8209</td>\n","      <td>4</td>\n","      <td>711.0</td>\n","      <td>20.1</td>\n","      <td>395.09</td>\n","      <td>18.06</td>\n","      <td>15.2</td>\n","    </tr>\n","    <tr>\n","      <th>489</th>\n","      <td>0.18337</td>\n","      <td>0.0</td>\n","      <td>27.74</td>\n","      <td>0</td>\n","      <td>0.609</td>\n","      <td>5.414</td>\n","      <td>98.3</td>\n","      <td>1.7554</td>\n","      <td>4</td>\n","      <td>711.0</td>\n","      <td>20.1</td>\n","      <td>344.05</td>\n","      <td>23.97</td>\n","      <td>7.0</td>\n","    </tr>\n","    <tr>\n","      <th>490</th>\n","      <td>0.20746</td>\n","      <td>0.0</td>\n","      <td>27.74</td>\n","      <td>0</td>\n","      <td>0.609</td>\n","      <td>5.093</td>\n","      <td>98.0</td>\n","      <td>1.8226</td>\n","      <td>4</td>\n","      <td>711.0</td>\n","      <td>20.1</td>\n","      <td>318.43</td>\n","      <td>29.68</td>\n","      <td>8.1</td>\n","    </tr>\n","    <tr>\n","      <th>491</th>\n","      <td>0.10574</td>\n","      <td>0.0</td>\n","      <td>27.74</td>\n","      <td>0</td>\n","      <td>0.609</td>\n","      <td>5.983</td>\n","      <td>98.8</td>\n","      <td>1.8681</td>\n","      <td>4</td>\n","      <td>711.0</td>\n","      <td>20.1</td>\n","      <td>390.11</td>\n","      <td>18.07</td>\n","      <td>13.6</td>\n","    </tr>\n","    <tr>\n","      <th>492</th>\n","      <td>0.11132</td>\n","      <td>0.0</td>\n","      <td>27.74</td>\n","      <td>0</td>\n","      <td>0.609</td>\n","      <td>5.983</td>\n","      <td>83.5</td>\n","      <td>2.1099</td>\n","      <td>4</td>\n","      <td>711.0</td>\n","      <td>20.1</td>\n","      <td>396.90</td>\n","      <td>13.35</td>\n","      <td>20.1</td>\n","    </tr>\n","    <tr>\n","      <th>493</th>\n","      <td>0.17331</td>\n","      <td>0.0</td>\n","      <td>9.69</td>\n","      <td>0</td>\n","      <td>0.585</td>\n","      <td>5.707</td>\n","      <td>54.0</td>\n","      <td>2.3817</td>\n","      <td>6</td>\n","      <td>391.0</td>\n","      <td>19.2</td>\n","      <td>396.90</td>\n","      <td>12.01</td>\n","      <td>21.8</td>\n","    </tr>\n","    <tr>\n","      <th>494</th>\n","      <td>0.27957</td>\n","      <td>0.0</td>\n","      <td>9.69</td>\n","      <td>0</td>\n","      <td>0.585</td>\n","      <td>5.926</td>\n","      <td>42.6</td>\n","      <td>2.3817</td>\n","      <td>6</td>\n","      <td>391.0</td>\n","      <td>19.2</td>\n","      <td>396.90</td>\n","      <td>13.59</td>\n","      <td>24.5</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>0.17899</td>\n","      <td>0.0</td>\n","      <td>9.69</td>\n","      <td>0</td>\n","      <td>0.585</td>\n","      <td>5.670</td>\n","      <td>28.8</td>\n","      <td>2.7986</td>\n","      <td>6</td>\n","      <td>391.0</td>\n","      <td>19.2</td>\n","      <td>393.29</td>\n","      <td>17.60</td>\n","      <td>23.1</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>0.28960</td>\n","      <td>0.0</td>\n","      <td>9.69</td>\n","      <td>0</td>\n","      <td>0.585</td>\n","      <td>5.390</td>\n","      <td>72.9</td>\n","      <td>2.7986</td>\n","      <td>6</td>\n","      <td>391.0</td>\n","      <td>19.2</td>\n","      <td>396.90</td>\n","      <td>21.14</td>\n","      <td>19.7</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>0.26838</td>\n","      <td>0.0</td>\n","      <td>9.69</td>\n","      <td>0</td>\n","      <td>0.585</td>\n","      <td>5.794</td>\n","      <td>70.6</td>\n","      <td>2.8927</td>\n","      <td>6</td>\n","      <td>391.0</td>\n","      <td>19.2</td>\n","      <td>396.90</td>\n","      <td>14.10</td>\n","      <td>18.3</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>0.23912</td>\n","      <td>0.0</td>\n","      <td>9.69</td>\n","      <td>0</td>\n","      <td>0.585</td>\n","      <td>6.019</td>\n","      <td>65.3</td>\n","      <td>2.4091</td>\n","      <td>6</td>\n","      <td>391.0</td>\n","      <td>19.2</td>\n","      <td>396.90</td>\n","      <td>12.92</td>\n","      <td>21.2</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>0.17783</td>\n","      <td>0.0</td>\n","      <td>9.69</td>\n","      <td>0</td>\n","      <td>0.585</td>\n","      <td>5.569</td>\n","      <td>73.5</td>\n","      <td>2.3999</td>\n","      <td>6</td>\n","      <td>391.0</td>\n","      <td>19.2</td>\n","      <td>395.77</td>\n","      <td>15.10</td>\n","      <td>17.5</td>\n","    </tr>\n","    <tr>\n","      <th>500</th>\n","      <td>0.22438</td>\n","      <td>0.0</td>\n","      <td>9.69</td>\n","      <td>0</td>\n","      <td>0.585</td>\n","      <td>6.027</td>\n","      <td>79.7</td>\n","      <td>2.4982</td>\n","      <td>6</td>\n","      <td>391.0</td>\n","      <td>19.2</td>\n","      <td>396.90</td>\n","      <td>14.33</td>\n","      <td>16.8</td>\n","    </tr>\n","    <tr>\n","      <th>501</th>\n","      <td>0.06263</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0</td>\n","      <td>0.573</td>\n","      <td>6.593</td>\n","      <td>69.1</td>\n","      <td>2.4786</td>\n","      <td>1</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>391.99</td>\n","      <td>9.67</td>\n","      <td>22.4</td>\n","    </tr>\n","    <tr>\n","      <th>502</th>\n","      <td>0.04527</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0</td>\n","      <td>0.573</td>\n","      <td>6.120</td>\n","      <td>76.7</td>\n","      <td>2.2875</td>\n","      <td>1</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>396.90</td>\n","      <td>9.08</td>\n","      <td>20.6</td>\n","    </tr>\n","    <tr>\n","      <th>503</th>\n","      <td>0.06076</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0</td>\n","      <td>0.573</td>\n","      <td>6.976</td>\n","      <td>91.0</td>\n","      <td>2.1675</td>\n","      <td>1</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>396.90</td>\n","      <td>5.64</td>\n","      <td>23.9</td>\n","    </tr>\n","    <tr>\n","      <th>504</th>\n","      <td>0.10959</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0</td>\n","      <td>0.573</td>\n","      <td>6.794</td>\n","      <td>89.3</td>\n","      <td>2.3889</td>\n","      <td>1</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>393.45</td>\n","      <td>6.48</td>\n","      <td>22.0</td>\n","    </tr>\n","    <tr>\n","      <th>505</th>\n","      <td>0.04741</td>\n","      <td>0.0</td>\n","      <td>11.93</td>\n","      <td>0</td>\n","      <td>0.573</td>\n","      <td>6.030</td>\n","      <td>80.8</td>\n","      <td>2.5050</td>\n","      <td>1</td>\n","      <td>273.0</td>\n","      <td>21.0</td>\n","      <td>396.90</td>\n","      <td>7.88</td>\n","      <td>11.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>506 rows × 14 columns</p>\n","</div>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"Rntb-1qWH_hv"},"source":["Here we will review 3 of the most common metrics for evaluating predictions on regression ML problems:\n","* [REGR-1] Mean Absolute Error\n","* [REGR-2] Mean Squared Error\n","* [REGR-3] $R^2$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SEl6bbYoH_hv"},"source":["## [REGR-1] Mean Absolute Error"]},{"cell_type":"markdown","metadata":{"id":"t13d5LAvH_hw"},"source":["The Mean Absolute Error (or MAE) is **the sum of the absolute differences between predictions and actual values**. "]},{"cell_type":"markdown","metadata":{"id":"fyzznaLS_rgP"},"source":["The example below demonstrates calculating mean absolute error on the house dataset."]},{"cell_type":"code","metadata":{"id":"1xoyhUzsH_hw","executionInfo":{"status":"ok","timestamp":1617856589018,"user_tz":-120,"elapsed":3338,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LinearRegression"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOPJHxVnH_h1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856589019,"user_tz":-120,"elapsed":3336,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"00a2a4a1-e99e-4d41-c38b-6e24c95a0523"},"source":["# Cross Validation Regression MAE\n","kfold = KFold(n_splits=10, random_state=7)\n","model = LinearRegression()\n","#\n","scoring = 'neg_mean_absolute_error'                                  # <---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"MAE: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["MAE: -0.337 (0.022)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yfhhJqhVH_h3"},"source":["A value of 0 indicates no error or perfect predictions. Like log-loss, this metric is inverted by\n","the `cross_val_score()` function."]},{"cell_type":"markdown","metadata":{"id":"k-pivqOUH_h4"},"source":["## [REGR-2] Mean Squared Error"]},{"cell_type":"markdown","metadata":{"id":"LL-RaegGH_h5"},"source":["The Mean Squared Error (or MSE) is much like the MAE in that **it provides a gross idea of the magnitude of error**. "]},{"cell_type":"markdown","metadata":{"id":"u5WMIXHeALJv"},"source":["\n","The example below provides a demonstration of calculating MSE."]},{"cell_type":"code","metadata":{"id":"AYdJ01hMH_h5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856589020,"user_tz":-120,"elapsed":3311,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"9ea6f904-ea21-4af8-f1d3-b343ec7b856c"},"source":["#num_folds = 10       # a remnant..\n","kfold = KFold(n_splits=10, random_state=7)\n","model = LinearRegression()\n","#\n","scoring = 'neg_mean_squared_error'\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"MSE: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["MSE: -0.163 (0.022)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SalKxNvNH_h8"},"source":["This metric too is inverted so that the results are increasing. "]},{"cell_type":"markdown","metadata":{"id":"nBBBy7LfH_h8"},"source":["## [REGR-3] $R^2$ metric"]},{"cell_type":"markdown","metadata":{"id":"6tZ8RVQmH_h9"},"source":["The $R^2$ (or R Squared) metric provides **an indication of the goodness of fit of a set of predictions to the actual values**.\n"]},{"cell_type":"markdown","metadata":{"id":"fWsHA29oAlky"},"source":["\n","The example below provides a demonstration of calculating the mean $R^2$ for a set of predictions."]},{"cell_type":"code","metadata":{"id":"r21teffhH_h9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617856589021,"user_tz":-120,"elapsed":3309,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"47e80a60-00db-435a-a4bc-04e3834b93e2"},"source":["# Cross Validation Regression R^2\n","kfold = KFold(n_splits=10, random_state=7)\n","model = LinearRegression()\n","#\n","scoring = 'r2'                                                     # <---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"R^2: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["R^2: 0.258 (0.118)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YPPk368jH_iA"},"source":["You can see the predictions have a poor fit to the actual values with a value closer to zero and less than 0.5."]},{"cell_type":"markdown","metadata":{"id":"_HZhOq_NH_iA"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"7qksn4XwH_iB"},"source":["What we did:\n","\n","* we discovered metrics that you can use to evaluate your ML algorithms. We learned about 3 classification metrics (Accuracy, LogLoss and AUC) and 2 convenience methods for classification prediction results (Confusion Matrix and Classification Report), as well as 3 metrics for regression problems (MAE, MSE, R2)."]}]}