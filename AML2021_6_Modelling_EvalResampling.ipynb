{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"AML2021_6_Modelling_EvalResampling.ipynb","provenance":[],"collapsed_sections":["6NyHWASMpbyq","_hXMsaB0wVxk","HMPTbXT1wVxx","k59kKKq_wVyA","zNCxbHA1wVyP","F0-YQZkAwVyW","WXGOB_6qwVyb","0jDFgpsWwVye","u6gwM2Z4wVyk","gmbTN4qiwVyo","L86TrKElwVyp","AvXL00WCwVyp"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DAcD2u6uwVxh"},"source":["# Evaluate the performance of ML algos with Resampling"]},{"cell_type":"markdown","metadata":{"id":"txViSnbzY6TR"},"source":["We are going to look at 4 different techniques that we can use to split up our training dataset and create useful estimates of performance for our ML algorithms:\n","\n","1. Split into Train and Test Sets\n","1. k-fold Cross-Validation\n","1. Leave One Out Cross-Validation\n","1. Repeated Random Test-Train Splits"]},{"cell_type":"markdown","metadata":{"id":"6NyHWASMpbyq"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"egGmPYr6pblR","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1616776068903,"user_tz":-60,"elapsed":1416,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"be453131-ca91-47c1-d51c-4c3cce89fd0e"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML2021Bas/main/datasets/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     preg  plas  pres  skin  test  mass   pedi  age  class\n","0       6   148    72    35     0  33.6  0.627   50      1\n","1       1    85    66    29     0  26.6  0.351   31      0\n","2       8   183    64     0     0  23.3  0.672   32      1\n","3       1    89    66    23    94  28.1  0.167   21      0\n","4       0   137    40    35   168  43.1  2.288   33      1\n","5       5   116    74     0     0  25.6  0.201   30      0\n","6       3    78    50    32    88  31.0  0.248   26      1\n","7      10   115     0     0     0  35.3  0.134   29      0\n","8       2   197    70    45   543  30.5  0.158   53      1\n","9       8   125    96     0     0   0.0  0.232   54      1\n","10      4   110    92     0     0  37.6  0.191   30      0\n","11     10   168    74     0     0  38.0  0.537   34      1\n","12     10   139    80     0     0  27.1  1.441   57      0\n","13      1   189    60    23   846  30.1  0.398   59      1\n","14      5   166    72    19   175  25.8  0.587   51      1\n","15      7   100     0     0     0  30.0  0.484   32      1\n","16      0   118    84    47   230  45.8  0.551   31      1\n","17      7   107    74     0     0  29.6  0.254   31      1\n","18      1   103    30    38    83  43.3  0.183   33      0\n","19      1   115    70    30    96  34.6  0.529   32      1\n","20      3   126    88    41   235  39.3  0.704   27      0\n","21      8    99    84     0     0  35.4  0.388   50      0\n","22      7   196    90     0     0  39.8  0.451   41      1\n","23      9   119    80    35     0  29.0  0.263   29      1\n","24     11   143    94    33   146  36.6  0.254   51      1\n","25     10   125    70    26   115  31.1  0.205   41      1\n","26      7   147    76     0     0  39.4  0.257   43      1\n","27      1    97    66    15   140  23.2  0.487   22      0\n","28     13   145    82    19   110  22.2  0.245   57      0\n","29      5   117    92     0     0  34.1  0.337   38      0\n","..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n","738     2    99    60    17   160  36.6  0.453   21      0\n","739     1   102    74     0     0  39.5  0.293   42      1\n","740    11   120    80    37   150  42.3  0.785   48      1\n","741     3   102    44    20    94  30.8  0.400   26      0\n","742     1   109    58    18   116  28.5  0.219   22      0\n","743     9   140    94     0     0  32.7  0.734   45      1\n","744    13   153    88    37   140  40.6  1.174   39      0\n","745    12   100    84    33   105  30.0  0.488   46      0\n","746     1   147    94    41     0  49.3  0.358   27      1\n","747     1    81    74    41    57  46.3  1.096   32      0\n","748     3   187    70    22   200  36.4  0.408   36      1\n","749     6   162    62     0     0  24.3  0.178   50      1\n","750     4   136    70     0     0  31.2  1.182   22      1\n","751     1   121    78    39    74  39.0  0.261   28      0\n","752     3   108    62    24     0  26.0  0.223   25      0\n","753     0   181    88    44   510  43.3  0.222   26      1\n","754     8   154    78    32     0  32.4  0.443   45      1\n","755     1   128    88    39   110  36.5  1.057   37      1\n","756     7   137    90    41     0  32.0  0.391   39      0\n","757     0   123    72     0     0  36.3  0.258   52      1\n","758     1   106    76     0     0  37.5  0.197   26      0\n","759     6   190    92     0     0  35.5  0.278   66      1\n","760     2    88    58    26    16  28.4  0.766   22      0\n","761     9   170    74    31     0  44.0  0.403   43      1\n","762     9    89    62     0     0  22.5  0.142   33      0\n","763    10   101    76    48   180  32.9  0.171   63      0\n","764     2   122    70    27     0  36.8  0.340   27      0\n","765     5   121    72    23   112  26.2  0.245   30      0\n","766     1   126    60     0     0  30.1  0.349   47      1\n","767     1    93    70    31     0  30.4  0.315   23      0\n","\n","[768 rows x 9 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>preg</th>\n","      <th>plas</th>\n","      <th>pres</th>\n","      <th>skin</th>\n","      <th>test</th>\n","      <th>mass</th>\n","      <th>pedi</th>\n","      <th>age</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>116</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>25.6</td>\n","      <td>0.201</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3</td>\n","      <td>78</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>88</td>\n","      <td>31.0</td>\n","      <td>0.248</td>\n","      <td>26</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>10</td>\n","      <td>115</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>35.3</td>\n","      <td>0.134</td>\n","      <td>29</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>197</td>\n","      <td>70</td>\n","      <td>45</td>\n","      <td>543</td>\n","      <td>30.5</td>\n","      <td>0.158</td>\n","      <td>53</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>8</td>\n","      <td>125</td>\n","      <td>96</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.232</td>\n","      <td>54</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4</td>\n","      <td>110</td>\n","      <td>92</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>37.6</td>\n","      <td>0.191</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>10</td>\n","      <td>168</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>38.0</td>\n","      <td>0.537</td>\n","      <td>34</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>10</td>\n","      <td>139</td>\n","      <td>80</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>27.1</td>\n","      <td>1.441</td>\n","      <td>57</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>189</td>\n","      <td>60</td>\n","      <td>23</td>\n","      <td>846</td>\n","      <td>30.1</td>\n","      <td>0.398</td>\n","      <td>59</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>5</td>\n","      <td>166</td>\n","      <td>72</td>\n","      <td>19</td>\n","      <td>175</td>\n","      <td>25.8</td>\n","      <td>0.587</td>\n","      <td>51</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>7</td>\n","      <td>100</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.0</td>\n","      <td>0.484</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0</td>\n","      <td>118</td>\n","      <td>84</td>\n","      <td>47</td>\n","      <td>230</td>\n","      <td>45.8</td>\n","      <td>0.551</td>\n","      <td>31</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7</td>\n","      <td>107</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29.6</td>\n","      <td>0.254</td>\n","      <td>31</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1</td>\n","      <td>103</td>\n","      <td>30</td>\n","      <td>38</td>\n","      <td>83</td>\n","      <td>43.3</td>\n","      <td>0.183</td>\n","      <td>33</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1</td>\n","      <td>115</td>\n","      <td>70</td>\n","      <td>30</td>\n","      <td>96</td>\n","      <td>34.6</td>\n","      <td>0.529</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>3</td>\n","      <td>126</td>\n","      <td>88</td>\n","      <td>41</td>\n","      <td>235</td>\n","      <td>39.3</td>\n","      <td>0.704</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>8</td>\n","      <td>99</td>\n","      <td>84</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>35.4</td>\n","      <td>0.388</td>\n","      <td>50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>7</td>\n","      <td>196</td>\n","      <td>90</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>39.8</td>\n","      <td>0.451</td>\n","      <td>41</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>9</td>\n","      <td>119</td>\n","      <td>80</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>29.0</td>\n","      <td>0.263</td>\n","      <td>29</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11</td>\n","      <td>143</td>\n","      <td>94</td>\n","      <td>33</td>\n","      <td>146</td>\n","      <td>36.6</td>\n","      <td>0.254</td>\n","      <td>51</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>10</td>\n","      <td>125</td>\n","      <td>70</td>\n","      <td>26</td>\n","      <td>115</td>\n","      <td>31.1</td>\n","      <td>0.205</td>\n","      <td>41</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>7</td>\n","      <td>147</td>\n","      <td>76</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>39.4</td>\n","      <td>0.257</td>\n","      <td>43</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>1</td>\n","      <td>97</td>\n","      <td>66</td>\n","      <td>15</td>\n","      <td>140</td>\n","      <td>23.2</td>\n","      <td>0.487</td>\n","      <td>22</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>13</td>\n","      <td>145</td>\n","      <td>82</td>\n","      <td>19</td>\n","      <td>110</td>\n","      <td>22.2</td>\n","      <td>0.245</td>\n","      <td>57</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>5</td>\n","      <td>117</td>\n","      <td>92</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>34.1</td>\n","      <td>0.337</td>\n","      <td>38</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>738</th>\n","      <td>2</td>\n","      <td>99</td>\n","      <td>60</td>\n","      <td>17</td>\n","      <td>160</td>\n","      <td>36.6</td>\n","      <td>0.453</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>739</th>\n","      <td>1</td>\n","      <td>102</td>\n","      <td>74</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>39.5</td>\n","      <td>0.293</td>\n","      <td>42</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>740</th>\n","      <td>11</td>\n","      <td>120</td>\n","      <td>80</td>\n","      <td>37</td>\n","      <td>150</td>\n","      <td>42.3</td>\n","      <td>0.785</td>\n","      <td>48</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>741</th>\n","      <td>3</td>\n","      <td>102</td>\n","      <td>44</td>\n","      <td>20</td>\n","      <td>94</td>\n","      <td>30.8</td>\n","      <td>0.400</td>\n","      <td>26</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>742</th>\n","      <td>1</td>\n","      <td>109</td>\n","      <td>58</td>\n","      <td>18</td>\n","      <td>116</td>\n","      <td>28.5</td>\n","      <td>0.219</td>\n","      <td>22</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>743</th>\n","      <td>9</td>\n","      <td>140</td>\n","      <td>94</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>32.7</td>\n","      <td>0.734</td>\n","      <td>45</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>744</th>\n","      <td>13</td>\n","      <td>153</td>\n","      <td>88</td>\n","      <td>37</td>\n","      <td>140</td>\n","      <td>40.6</td>\n","      <td>1.174</td>\n","      <td>39</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>745</th>\n","      <td>12</td>\n","      <td>100</td>\n","      <td>84</td>\n","      <td>33</td>\n","      <td>105</td>\n","      <td>30.0</td>\n","      <td>0.488</td>\n","      <td>46</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>746</th>\n","      <td>1</td>\n","      <td>147</td>\n","      <td>94</td>\n","      <td>41</td>\n","      <td>0</td>\n","      <td>49.3</td>\n","      <td>0.358</td>\n","      <td>27</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>747</th>\n","      <td>1</td>\n","      <td>81</td>\n","      <td>74</td>\n","      <td>41</td>\n","      <td>57</td>\n","      <td>46.3</td>\n","      <td>1.096</td>\n","      <td>32</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>748</th>\n","      <td>3</td>\n","      <td>187</td>\n","      <td>70</td>\n","      <td>22</td>\n","      <td>200</td>\n","      <td>36.4</td>\n","      <td>0.408</td>\n","      <td>36</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>749</th>\n","      <td>6</td>\n","      <td>162</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24.3</td>\n","      <td>0.178</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>750</th>\n","      <td>4</td>\n","      <td>136</td>\n","      <td>70</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>31.2</td>\n","      <td>1.182</td>\n","      <td>22</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>751</th>\n","      <td>1</td>\n","      <td>121</td>\n","      <td>78</td>\n","      <td>39</td>\n","      <td>74</td>\n","      <td>39.0</td>\n","      <td>0.261</td>\n","      <td>28</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>752</th>\n","      <td>3</td>\n","      <td>108</td>\n","      <td>62</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>26.0</td>\n","      <td>0.223</td>\n","      <td>25</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>753</th>\n","      <td>0</td>\n","      <td>181</td>\n","      <td>88</td>\n","      <td>44</td>\n","      <td>510</td>\n","      <td>43.3</td>\n","      <td>0.222</td>\n","      <td>26</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>754</th>\n","      <td>8</td>\n","      <td>154</td>\n","      <td>78</td>\n","      <td>32</td>\n","      <td>0</td>\n","      <td>32.4</td>\n","      <td>0.443</td>\n","      <td>45</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>755</th>\n","      <td>1</td>\n","      <td>128</td>\n","      <td>88</td>\n","      <td>39</td>\n","      <td>110</td>\n","      <td>36.5</td>\n","      <td>1.057</td>\n","      <td>37</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>756</th>\n","      <td>7</td>\n","      <td>137</td>\n","      <td>90</td>\n","      <td>41</td>\n","      <td>0</td>\n","      <td>32.0</td>\n","      <td>0.391</td>\n","      <td>39</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>757</th>\n","      <td>0</td>\n","      <td>123</td>\n","      <td>72</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>36.3</td>\n","      <td>0.258</td>\n","      <td>52</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>758</th>\n","      <td>1</td>\n","      <td>106</td>\n","      <td>76</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>37.5</td>\n","      <td>0.197</td>\n","      <td>26</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>759</th>\n","      <td>6</td>\n","      <td>190</td>\n","      <td>92</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>35.5</td>\n","      <td>0.278</td>\n","      <td>66</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>760</th>\n","      <td>2</td>\n","      <td>88</td>\n","      <td>58</td>\n","      <td>26</td>\n","      <td>16</td>\n","      <td>28.4</td>\n","      <td>0.766</td>\n","      <td>22</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>761</th>\n","      <td>9</td>\n","      <td>170</td>\n","      <td>74</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>44.0</td>\n","      <td>0.403</td>\n","      <td>43</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>762</th>\n","      <td>9</td>\n","      <td>89</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22.5</td>\n","      <td>0.142</td>\n","      <td>33</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>763</th>\n","      <td>10</td>\n","      <td>101</td>\n","      <td>76</td>\n","      <td>48</td>\n","      <td>180</td>\n","      <td>32.9</td>\n","      <td>0.171</td>\n","      <td>63</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>764</th>\n","      <td>2</td>\n","      <td>122</td>\n","      <td>70</td>\n","      <td>27</td>\n","      <td>0</td>\n","      <td>36.8</td>\n","      <td>0.340</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>765</th>\n","      <td>5</td>\n","      <td>121</td>\n","      <td>72</td>\n","      <td>23</td>\n","      <td>112</td>\n","      <td>26.2</td>\n","      <td>0.245</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>766</th>\n","      <td>1</td>\n","      <td>126</td>\n","      <td>60</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.1</td>\n","      <td>0.349</td>\n","      <td>47</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>767</th>\n","      <td>1</td>\n","      <td>93</td>\n","      <td>70</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>30.4</td>\n","      <td>0.315</td>\n","      <td>23</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>768 rows × 9 columns</p>\n","</div>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"_hXMsaB0wVxk"},"source":["## 1. Split into Train and Test Sets"]},{"cell_type":"markdown","metadata":{"id":"_8RaMNPVccVP"},"source":["The simplest method that we can use to evaluate the performance of a ML algorithm is to separate the dataset, and use (at least) different training and testing datasets (e.g. 2/3 and 1/3, but choices may vary).\n","\n","This algorithm evaluation technique is very fast, and has pros and cons:\n","* _Pro_. Ideal for large datasets. Fast (so use it for algos slow in training)\n","* _Con_. High variance.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"2Qu_ml5owVxl","executionInfo":{"status":"ok","timestamp":1616776271531,"user_tz":-60,"elapsed":1837,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"yzT3ldEtwVxo","executionInfo":{"status":"ok","timestamp":1616776274156,"user_tz":-60,"elapsed":608,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVBhj9VXwVxr","executionInfo":{"status":"ok","timestamp":1616776656271,"user_tz":-60,"elapsed":489,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["test_size = 0.33\n","seed = 7"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQ2GUpzjwVxu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616776495567,"user_tz":-60,"elapsed":618,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"69b30655-b858-42bf-ae3a-02c34071bb6f"},"source":["# Evaluate using a train and a test set\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n","model = LogisticRegression()            \n","model.fit(X_train, Y_train)             \n","result = model.score(X_test, Y_test)    \n","print(\"Accuracy: %.3f%%\" % (result*100.0))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Accuracy: 75.591%\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Q9OaA7TQfw7I"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"HMPTbXT1wVxx"},"source":["### <font color='red'>Exercise 1</font>"]},{"cell_type":"markdown","metadata":{"id":"lBLCtCkawVxx"},"source":["Try to change the seed, and re-train. Does accuracy change? Is it reproducible for a a fixed seed? for different seeds, could you measure its variance? (up to your curiosity here, but no need to do more here than just few tries and get a feeling.. but you can do more and clever tests..)"]},{"cell_type":"code","metadata":{"id":"m2qqRJk-xhJH"},"source":["# type your code below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTOf-eg8fx-i"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"k59kKKq_wVyA"},"source":["### <font color='red'>Exercise 2</font>"]},{"cell_type":"markdown","metadata":{"id":"kflKRgO0wVyA"},"source":["What happens if I check accuracy on the _train_ set (conceptually wrong)? Do I see something different or not? What is the drawback if I do this mistake?"]},{"cell_type":"code","metadata":{"id":"nUH8RXqB09R_"},"source":["# type your code below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WS-tPxm9fzHJ"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"zNCxbHA1wVyP"},"source":["### <font color='red'>Exercise 3</font>"]},{"cell_type":"markdown","metadata":{"id":"EaSn-xI3wVyP"},"source":["What if change the training/test ratio?"]},{"cell_type":"code","metadata":{"id":"6mnkaabe0-kQ"},"source":["# type your code below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"81jkbtANgE1Y"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"F0-YQZkAwVyW"},"source":["## 2. K-fold Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"6L7CDFjkzQYn"},"source":["It works by **splitting the dataset into k-parts** (e.g. $k=5$ or $k=10$). Each split of the data is called a $fold$. The algorithm is trained on $k-1$ folds (with 1 held back), and then tested on the held-back fold. This is also repeated, so that _each_ fold of the dataset is given a chance to be the held-back test set. So you repeat it k times. After running cross-validation you end up with $k$ different performance scores that you can summarize using a mean and a standard deviation. \n"]},{"cell_type":"markdown","metadata":{"id":"RdXgfR4PirTJ"},"source":["**The choice of $k$ is a trade-off** between reasonably large size of each test partition, and a number that allows enough repetitions of the train-test evaluation of the algorithm.\n","\n","**$k$ values of $3$, $5$ and $10$ are common** (at least for modest-size datasets in the thousands or tens of thousands of records). In the example below we use 10-fold cross-validation."]},{"cell_type":"code","metadata":{"id":"Y9tS9Sb0wVyW","executionInfo":{"status":"ok","timestamp":1616776868727,"user_tz":-60,"elapsed":521,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score   # <---\n","from sklearn.linear_model import LogisticRegression"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLE_nHF5wVyZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616776936004,"user_tz":-60,"elapsed":1233,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"6d0cbc0b-eba7-4a33-eeb0-8f228911d90c"},"source":["# Evaluate using Cross Validation\n","num_folds = 100\n","seed = 7\n","\n","kfold = KFold(n_splits=num_folds, random_state=seed)\n","model = LogisticRegression()\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Accuracy: 77.196% (15.181%)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3_tYK3AWwVyb"},"source":["You can see that we report both the mean and the standard deviation of the performance measure.\n"]},{"cell_type":"markdown","metadata":{"id":"yN1DWy2FkWH_"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"WXGOB_6qwVyb"},"source":["### <font color='red'>Exercise 4</font>"]},{"cell_type":"markdown","metadata":{"id":"RbpMa5EKwVyc"},"source":["<div class=\"alert alert-block alert-info\">\n","What if I change the nb folds?\n","</div>"]},{"cell_type":"code","metadata":{"id":"2O1NeZfW0_ui"},"source":["# type your code below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jDFgpsWwVye"},"source":["## 3. Leave One Out Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"TsI_mVLWwVyf"},"source":["You can configure cross-validation so that the size of the fold is 1 ($k=n$, i.e. $k$ is set to the number of observations in your dataset). "]},{"cell_type":"code","metadata":{"id":"80lH8gNLwVyf","executionInfo":{"status":"ok","timestamp":1616777048813,"user_tz":-60,"elapsed":645,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.model_selection import LeaveOneOut       # <---\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LogisticRegression"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"B92zPwKwwVyh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616777066100,"user_tz":-60,"elapsed":5917,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"c879a97f-2810-4578-db38-1ffe81dbb372"},"source":["# Evaluate using Leave One Out Cross Validation\n","loocv = LeaveOneOut()\n","model = LogisticRegression()\n","results = cross_val_score(model, X, Y, cv=loocv)\n","print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Accuracy: 76.823% (42.196%)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x4gRjJOvwVyj"},"source":["(*NOTE: probably not so visible in this small example, but the time it took to run this is larger than the previous one..*)\n","\n","You can see in the standard deviation that the score has **higher variance** than the k-fold cross-validation results described above."]},{"cell_type":"markdown","metadata":{"id":"u6gwM2Z4wVyk"},"source":["## 4. Repeated Random Test-Train Splits"]},{"cell_type":"markdown","metadata":{"id":"ku79ofdQz-0B"},"source":["Another variation on k-fold cross-validation is to **create a random split of the data** like the train/test split described above, but **repeat multiple times the process of splitting and evaluation of the algorithm**, like cross-validation.\n","\n","The example below splits the data into a 67%/33% train/test split and repeats the process 10 times."]},{"cell_type":"code","metadata":{"id":"ZKBoxgB3wVyk","executionInfo":{"status":"ok","timestamp":1616777169349,"user_tz":-60,"elapsed":503,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}}},"source":["from sklearn.model_selection import ShuffleSplit      # <---\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LogisticRegression"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKVeSQ3SwVym","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616777178192,"user_tz":-60,"elapsed":1090,"user":{"displayName":"Daniele Bonacorsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWBsRbQFtIRjciBi-aFy9qOKzprHfbmXdS8BQD0A=s64","userId":"01397661520201218305"}},"outputId":"24404071-7577-402f-e821-5e86648fcdad"},"source":["# Evaluate using Shuffle Split Cross Validation\n","n_splits = 100\n","test_size = 0.33\n","seed = 7\n","\n","kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n","model = LogisticRegression()\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Accuracy: 76.795% (2.311%)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tni5i-f_wVyo"},"source":["We can see that in this case the distribution of the performance measure is on par with\n","k-fold cross-validation above."]},{"cell_type":"markdown","metadata":{"id":"gmbTN4qiwVyo"},"source":["## OK, fine, but.. what techniques to use when?!?"]},{"cell_type":"markdown","metadata":{"id":"s0ePQcYT0IZX"},"source":["Discussion at the lecture."]},{"cell_type":"markdown","metadata":{"id":"L86TrKElwVyp"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"9Wmx7bsxwVyp"},"source":["What we did:\n","\n","* we discovered 4 statistical techniques that we can use to estimate the performance of ML algorithms, called Resampling. "]},{"cell_type":"markdown","metadata":{"id":"AvXL00WCwVyp"},"source":["## What's next "]},{"cell_type":"markdown","metadata":{"id":"KaIOetr1wVyq"},"source":["Now we will see how you can evaluate the performance of classification and regression algorithms using a suite of different metrics and built in evaluation reports."]}]}