{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"name":"AML2021_5_DataPrep_FeatSelect.ipynb","provenance":[],"collapsed_sections":["uR9VV3FARN5F","6NyHWASMpbyq","Kg2NJcPkRN5H","cj-JUxDxRN5u","IO_oKcAkRN59","yqG9Uyk8RN6L","ihX3gbMNRN6k","5OuvxuzuRN6l"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Op2yF_XGRN5D"},"source":["# Feature Selection for ML"]},{"cell_type":"markdown","metadata":{"id":"3Q9aUfG6RN5E"},"source":["We discuss feature selection techniques that you can use to prepare your ML data in Python with scikit-learn. \n","\n","Focus will be on:\n","1. Univariate Selection\n","2. Recursive Feature Elimination\n","3. Feature Importance\n","\n","Then we look at another approach: Principal Component Analysis (not plenty of details in this part of the course)"]},{"cell_type":"markdown","metadata":{"id":"uR9VV3FARN5F"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"3ubyEy6FRN5G"},"source":["Keep an eye, in the following, to 3 possible goals/benefits of a clever feature selection tactics:\n","\n","1. ***Reduction of Overfitting***\n","1. ***Improvement of Accuracy***\n","1. ***Reduction of Training Time***\n"]},{"cell_type":"markdown","metadata":{"id":"rsBL2p7Mpo7W"},"source":["\n","More about feature selection with scikit-learn can be found [here](http://scikit-learn.org/stable/modules/feature_selection.html)."]},{"cell_type":"markdown","metadata":{"id":"6NyHWASMpbyq"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"egGmPYr6pblR"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML2021Bas/main/datasets/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPZ8swohksne"},"source":["data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kg2NJcPkRN5H"},"source":["## 1. Univariate Selection"]},{"cell_type":"markdown","metadata":{"id":"BaoeTaNARN5I"},"source":["The scikit-learn library provides the `SelectKBest` class (info [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)) that can be used with a suite of different statistical tests to select a specific number of features. Apart from `SelectKBest`, you may use `SelectPercentile` or `GenericUnivariateSelect` (check documentation).\n","\n","The example below uses the chi-squared (chi2) statistical test for non-negative features to select 4 of the best features from the Pima Indians onset of diabetes dataset."]},{"cell_type":"code","metadata":{"id":"evZ0LUW9RN5K"},"source":["from numpy import set_printoptions\n","\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eTs-DqyQRN5P"},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xavljlu0lAR3"},"source":["X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jbOHslallCfW"},"source":["Y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z17GQmcbRN5T"},"source":["# chi2 to select the best k=..\n","test = SelectKBest(score_func=chi2, k=3)\n","fit = test.fit(X, Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xXadTD4RN5X"},"source":["# summarize scores\n","set_printoptions(precision=3)\n","print(fit.scores_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8AVEa39RN5f"},"source":["names[:-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lNl3O2YFRN5j"},"source":["So, the **k(=3) best ones** seems to be: **plas, test** (and perhaps **age**, but much worse)"]},{"cell_type":"markdown","metadata":{"id":"oFY3cxqIlNR4"},"source":["[NOTE]: If you want to transform it, you see that change of shape.. (careful, this is powerful..) "]},{"cell_type":"code","metadata":{"id":"anIiC5delLnZ"},"source":["X_new = SelectKBest(score_func=chi2, k=3).fit_transform(X, Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQduw3nklLkO"},"source":["X_new.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v9xUrPpjlLhZ"},"source":["X_new"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQ8cZXbYlz3p"},"source":["Let's go back on track."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"q6IpDyssRN5k"},"source":["# summarize selected features\n","X_new1 = fit.transform(X)\n","print(X_new1[0:10,:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRlOWtD7mU_I"},"source":["You can do all the above in one shot with `fit_transform` (careful, often this is dangerous..)"]},{"cell_type":"code","metadata":{"id":"7waiPBLXmU7z"},"source":["X_new2 = SelectKBest(score_func=chi2, k=3).fit_transform(X,Y)\n","print(X_new2[0:10,:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cj-JUxDxRN5u"},"source":["## 2. Recursive Feature Elimination"]},{"cell_type":"markdown","metadata":{"id":"8SSPSZcLRN5v"},"source":["\n","More about the RFE class in the scikit-learn documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)."]},{"cell_type":"code","metadata":{"id":"FP-jW8WKRN5w"},"source":["from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Oc1qtNuiRN50"},"source":["model = LogisticRegression()\n","rfe = RFE(model, 3)    # my choice: seek for 3 features\n","fit = rfe.fit(X, Y)\n","print(\"Num Features: %d\" % fit.n_features_)\n","print(\"Selected Features: %s\" % fit.support_)\n","print(\"Feature Ranking: %s\" % fit.ranking_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"JlPDPjo0RN53"},"source":["names[:-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ErQenA9ZRN57"},"source":["You can see that RFE has chosen the **top 3 features** as **preg, mass, pedi**."]},{"cell_type":"markdown","metadata":{"id":"IO_oKcAkRN59"},"source":["## 3. Feature Importance"]},{"cell_type":"markdown","metadata":{"id":"tQRj9sCKRN59"},"source":["\n","More about the ExtraTreesClassifier class can be found in the scikit-learn API [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)."]},{"cell_type":"code","metadata":{"id":"juED2qQFRN5-"},"source":["from sklearn.ensemble import ExtraTreesClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9qC1hbGRN6B"},"source":["# Feature Importance with Extra Trees Classifier\n","model = ExtraTreesClassifier()\n","model.fit(X, Y)\n","print(model.feature_importances_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOzdbD0URN6F"},"source":["names[:-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdFqygZMRN6I"},"source":["**The larger the score, the more important the attribute**. The scores suggest at the importance of **plas** and perhaps also **mass, age**."]},{"cell_type":"markdown","metadata":{"id":"yqG9Uyk8RN6L"},"source":["## Another approach: Principal Component Analysis"]},{"cell_type":"markdown","metadata":{"id":"ywrBymoVRN6M"},"source":["\n","\n","More about the PCA class in scikit-learn can be found in its API documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."]},{"cell_type":"code","metadata":{"id":"Q0ujqO0sRN6N"},"source":["from sklearn.decomposition import PCA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"KMEyi5QLRN6a"},"source":["# Feature Extraction with PCA\n","pca = PCA(n_components=3)\n","fit = pca.fit(X)\n","# summarize components\n","print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"aXKNLuSlRN6f"},"source":["print(fit.components_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dnqlWtnLRN6j"},"source":["You can see that the transformed dataset (3 principal components) bear little resemblance to the source data! It is a completely different approach, valuable mainly if you need to reduce the dimensions and the complexity of the problem."]},{"cell_type":"markdown","metadata":{"id":"55hqcH8iIB_I"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"tfv3IgWJRN6J"},"source":["### <font color='red'>Exercise</font>"]},{"cell_type":"markdown","metadata":{"id":"x7ZAW_XxRN6K"},"source":["Can you \"compare\" the first 3 methods above and draw any conclusions on the features you would eventually pick?"]},{"cell_type":"markdown","metadata":{"id":"A5bGAvBnIC0A"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"ihX3gbMNRN6k"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"DUoPeiP5RN6k"},"source":["What we did:\n","\n","* we explored feature selection for preparing ML data in Python with scikit-learn, and we discovered 4 different automatic feature selection techniques."]},{"cell_type":"markdown","metadata":{"id":"5OuvxuzuRN6l"},"source":["## What's next "]},{"cell_type":"markdown","metadata":{"id":"_5iQYc30RN6m"},"source":["Now we will start looking at how to evaluate ML algorithms on your dataset, starting from discovering resampling methods that can be used to estimate the performance of a ML algorithm on unseen data."]}]}